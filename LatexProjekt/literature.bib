@INPROCEEDINGS{Bluhm:Sonar:2009,
  author = {Andreas Bluhm and Jens Eickmeyer and Tobias Feith and Nikita Mattar
	and Thies Pfeiffer},
  title = {{E}xploration von sozialen {N}etzwerken im {3D} {R}aum am {B}eispiel
	von $\mbox{SoN}\forall \mbox{R\ }$ f\"ur {L}ast.fm},
  booktitle = {Virtuelle und Erweiterte Realit\"at - Sechster Workshop der GI-Fachgruppe
	VR/AR},
  year = {2009},
  editor = {Andreas Gerndt and Marc Erich Latoschik},
  pages = {269--280},
  address = {Aachen},
  publisher = {Shaker Verlag}
}

@article{1,
  abstract     = {Industrie  4.0  (English  translation:  Industry  4.0) stands for functional integration, dynamic reorganization, and resource  efficiency.  Technical  advances  in  control  and  communication create infrastructures that handle more and more tasks automatically. As a result, the complexity of today’s and future  technical  systems  is  hidden  from  the  user.  These  advances, however, come with distinct challenges for user interface design.  A central  question is: how to  empower  users  to understand, monitor, and control the automated processes of Industrie 4.0? Addressing these design challenges requires a full  integration  of user-centered  design  (UCD) processes  into the  development  process.  This  paper  discusses  flexible  but powerful methods for usability and user experience engineering in the context of Industrie 4.0.},
  author       = {Pfeiffer, Thies and Hellmers, Jens and Schön, Eva-Maria and Thomaschewski, Jörg},
  journal      = {IEEE Proceedings Special Issue on Cyberphysical Systems},
  keyword      = {user centered design, industry 4.0, eye tracking, agile},
  number       = {5},
  pages        = {986 -- 996},
  publisher    = {IEEE},
  title        = {{Empowering User Interfaces for the Industry 4.0}},
  doi          = {10.1109/JPROC.2015.2508640  },
  volume       = {104},
  year         = {2016},
}

@inproceedings{2,
  abstract     = {While display quality and rendering for Head-Mounted-Displays (HMDs) has increased in quality and performance, the interaction capabilities with these devices are still very limited or relying on expensive technology. Current experiences offered for mobile HMDs often stick to dome-like looking around, automatic or gaze-triggered movement, or flying techniques. 

We developed an easy to use walking-in-place technique that does not require additional hardware to enable basic navigation, such as walking, running, or jumping, in virtual environments. Our approach is based on the analysis of data from the inertial unit embedded in mobile HMDs. In a first prototype realized for the Samsung Galaxy Gear VR we detect steps and jumps. A user study shows that users novice to virtual reality easily pick up the method. In comparison to a classic input device, using our walking-in-place technique study participants felt more present in the virtual environment and preferred our method for exploration of the virtual world. },
  author       = {Pfeiffer, Thies and Schmidt, Aljoscha and Renner, Patrick},
  booktitle    = {IEEE Virtual Reality 2016},
  keyword      = {Virtual Reality, Ultra Mobile Head Mounted Displays, Navigation, Walking-in-place},
  publisher    = {IEEE},
  title        = {{Detecting Movement Patterns from Inertial Data of a Mobile Head-Mounted-Display for Navigation via Walking-in-Place}},
  year         = {2016},
}

@inproceedings{3,
  abstract     = {With the launch of ultra-portable systems, mobile eye tracking finally has the potential to become mainstream. While eye movements on their own can already be used to identify human activities, such as reading or walking, linking eye movements to objects in the environment provides even deeper insights into human cognitive processing.

We present a model-based approach for the identification of fixated objects in three-dimensional environments. For evaluation, we compare the automatic labelling of fixations with those performed by human annotators. In addition to that, we show how the approach can be extended to support moving targets, such as individual limbs or faces of human interaction partners. The approach also scales to studies using multiple mobile eye-tracking systems in parallel.

The developed system supports real-time attentive systems that make use of eye tracking as means for indirect or direct human-computer interaction as well as off-line analysis for basic research purposes and usability studies. },
  author       = {Pfeiffer, Thies and Renner, Patrick and Pfeiffer-Leßmann, Nadine},
  booktitle    = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \& Applications},
  keyword      = {Eyetracking, Gaze-based Interaction},
  location     = {Charleston, SC, USA},
  pages        = {189--196},
  publisher    = {ACM Press},
  title        = {{EyeSee3D 2.0: Model-based Real-time Analysis of Mobile Eye-Tracking in Static and Dynamic Three-Dimensional Scenes}},
  doi          = {10.1145/2857491.2857532},
  year         = {2016},
}

@inproceedings{4,
  abstract     = {Heat maps, or more generally, attention maps or saliency maps are an often used technique to visualize eye-tracking data. With heat maps qualitative information about visual processing can be easily visualized and communicated between experts and laymen. They are thus a versatile tool for many disciplines, in particular for usability engineering, and are often used to get a first overview about recorded eye-tracking data.

Today, heat maps are typically generated for 2D stimuli that have been presented on a computer display. In such cases the mapping of overt visual attention on the stimulus is rather straight forward and the process is well understood. However, when turning towards mobile eye tracking and eye tracking in 3D virtual environments, the case is much more complicated. 

In the first part of the paper, we discuss several challenges that have to be considered in 3D environments, such as changing perspectives, multiple viewers, object occlusions, depth of fixations, or dynamically moving objects. In the second part, we present an approach for the generation of 3D heat maps addressing the above mentioned issues while working in real-time. Our visualizations provide high-quality output for multi-perspective eye-tracking recordings of visual attention in 3D environments.},
  author       = {Pfeiffer, Thies and Memili, Cem},
  booktitle    = {Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research \& Applications},
  keyword      = {Eyetracking, Gaze-based Interaction},
  location     = {Charleston, SC, USA},
  pages        = {95--102},
  publisher    = {ACM Press},
  title        = {{Model-based Real-time Visualization of Realistic Three-Dimensional Heat Maps for Mobile Eye Tracking and Eye Tracking in Virtual Reality}},
  doi          = {10.1145/2857491.2857541},
  year         = {2016},
}

@inproceedings{5,
  abstract     = {Virtual Reality (VR) has the potential to support motor learning in ways exceeding beyond the possibilities provided by real world environments. New feedback mechanisms can be implemented that support motor learning during the performance of the trainee and afterwards as a performance review. As a consequence, VR environments excel in controlled evaluations, which has been proven in many other application scenarios.

However, in the context of motor learning of complex tasks, including full-body movements, questions regarding the main technical parameters of such a system, in particular that of the required maximum latency, have not been addressed in depth. To fill this gap, we propose a set of requirements towards VR systems for motor learning, with a special focus on motion capturing and rendering. We then assess and evaluate state-of-the-art techniques and technologies for motion capturing and rendering, in order to provide data on latencies for different setups. We focus on the end-to-end latency of the overall system, and present an evaluation of an exemplary system that has been developed to meet these requirements.
},
  author       = {Waltemate, Thomas and Hülsmann, Felix and Pfeiffer, Thies and Kopp, Stefan and Botsch, Mario},
  booktitle    = {Proceedings of the 21st ACM Symposium on Virtual Reality Software and Technology},
  keyword      = {low-latency, motor learning, virtual reality},
  location     = {Beijing, China},
  pages        = {139--147},
  publisher    = {ACM},
  title        = {{Realizing a Low-latency Virtual Reality Environment for Motor Learning}},
  doi          = {10.1145/2821592.2821607},
  year         = {2015},
}

@inproceedings{6,
  abstract     = {Visual attention can be a viable source of information to assess human behaviors in many different contexts, from human-computer interaction, over sports or social interactions, to complex working environments, such as to be found in the context of Industry 4.0. In such scenarios in which the user is able to walk around freely, mobile eye-tracking systems are used to record eye movements, which are then mapped onto an ego-perspective video. The analysis of such recordings then requires large efforts for manually annotating the recorded videos on a frame-by-frame basis to label the fixations based on their locations to the target objects present in the video. First, we present a method to record eye movements in 3D scenarios and annotate fixations with corresponding labels for the objects of interest in real-time 2. For this purpose, we rely on computer-vision methods for the detection of the camera position and orientation in the world. Based on a coarse 3D model of the environment, representing the 3D areas of interest, fixations are mapped to areas of interest. As a result, we can identify the position of the fixation in terms of local object coordinates for each relevant object of interest. Second, we present a method for real-time creation and visualization of heatmaps for 3D objects 1. Based on a live-streaming of the recorded and analyzed eye movements, our solution renders heatmaps on top of the object s urfaces. The resulting visualizations are more realistic than standard 2D heatmaps, in that we consider occlusions, depth of focus and dynamic moving objects. Third, we present a new method which allows us to aggregate fixations on a per object basis, e.g. similar to regions/areas of interest. This allows us to transfer existing methods of analysis to 3D environments. We present examples from a virtual supermarket, a study on social interactions between two humans, examples from real-time gaze mapping on body parts of a moving humans and from studying 3D prototypes in a virtual reality environment.},
  author       = {Pfeiffer, Thies and Memili, Cem and Renner, Patrick},
  booktitle    = {Proceedings of the 2nd International Workshop on Solutions for Automatic Gaze Data Analysis 2015 (SAGA 2015)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {eye tracking},
  location     = {Bielefeld},
  pages        = {11--12},
  publisher    = {eCollections Bielefeld University},
  title        = {{Capturing and Visualizing Eye Movements in 3D Environments}},
  doi          = {10.2390/biecoll-saga2015_3},
  year         = {2015},
}

@misc{7,
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {eye tracking},
  location     = {Bielefeld},
  publisher    = {eCollections Bielefeld University},
  title        = {{Proceedings of the 2nd International Workshop on Solutions for Automatic Gaze Data Analysis 2015 (SAGA 2015)}},
  doi          = {10.2390/biecoll-saga2015_21},
  year         = {2015},
}

@inproceedings{8,
  abstract     = {Every now and then there are situations in which we are not sure how to proceed and thus are seeking for help. For example, choosing the best product out of dozens of different brands in a supermarket can be difficult, especially when following a specific diet. There are, however, also people who have problems with decision making or sequencing actions in everyday life, e.g. because they suffer from dementia. In such situations, it may be welcomed when there is someone around noticing our problem and offering help. In more private situations, e.g. in the bathroom, help in shape of a human being cannot be expected or even is not welcomed. Our research focuses on the design of mobile assistive systems which could assist in everyday life activities. Such a system needs to detect situations of helplessness, identify the interaction context, conclude what would be an appropriate assistance, before finally engaging in interaction with the user.},
  author       = {Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {SAGA 2015: 2nd International Workshop on Solutions for Automatic Gaze Data Analysis},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {eye tracking, gaze-based interaction, ADAMAAS},
  location     = {Bielefeld},
  pages        = {14--15},
  publisher    = {eCollections Bielefeld University},
  title        = {{Online Visual Attention Monitoring for Mobile Assistive Systems}},
  doi          = {10.2390/biecoll-saga2015_6},
  year         = {2015},
}

@inproceedings{9,
  abstract     = {Eye gaze plays an important role in human communication. One foundational skill in human social interaction is joint attention which is receiving increased interest in particular in the area of human-agent or human-robot interaction. We are focusing here on patterns of gaze interaction that emerge in the process of establishing joint attention. The approach, however, should be applicable to many other aspects of social communication in which eye gaze plays an important role. Attention has been characterized as an increased awareness 1 and intentionally directed perception 2 and is judged to be crucial for goal-directed behavior. Joint attention can be defined as simultaneously allocating attention to a target as a consequence of attending to each other's attentional states 3. In other words: Interlocutors have to deliberatively focus on the same target while being mutually aware of sharing their focus of attention 2 4.},
  author       = {Pfeiffer-Leßmann, Nadine and Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {SAGA 2015: 2nd International Workshop on Solutions for Automatic Gaze Data Analysis},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {joint attention, eye tracking},
  location     = {Bielefeld},
  pages        = {27--29},
  publisher    = {eCollections Bielefeld University},
  title        = {{Analyzing Patterns of Eye Movements in Social Interactions}},
  doi          = {10.2390/biecoll-saga2015_10},
  year         = {2015},
}

@article{10,
  abstract     = {Our research aims at cognitive modelling of joint attention for artificial agents, such as virtual
agents or robots. With the current study, we are focusing on the observation of interaction
patterns of joint attention and their time course. For this, we recorded and analyzed
twenty sessions of two interacting participants using two mobile binocular eye-tracking
systems.
A key contribution of our work addresses methodological aspects of mobile eye-tracking
studies with scene camera recordings in general. The standard procedure for the analysis of
such gaze videos requires a manual annotation. This time consuming process often exceeds
multiple times the duration of the original recordings (e.g. 30 times). This doubles if, as in
our case, the gaze of two interlocutors is recorded simultaneously.
In our approach, we build upon our EyeSee3D approach for marker-based tracking and
registration of the environment and a 3D reconstruction of the relevant stimuli. We extend
upon the previous approach in supporting more than one participant and dynamically
changing stimuli, here the faces and eyes of the interlocutors. The full analysis of the time
course of both interlocutor’s gaze is done in real-time and available for analysis right after
the experiment without the requirement for manual annotation.},
  author       = {Renner, Patrick and Pfeiffer, Thies and Pfeiffer-Leßmann, Nadine},
  journal      = {Abstracts of the 18th European Conference on Eye Movements},
  keyword      = {eye tracking, joint attention, 3D},
  pages        = {116--116},
  title        = {{Automatic Analysis of a Mobile Dual Eye-Tracking Study on Joint Attention}},
  year         = {2015},
}

@article{11,
  abstract     = {Current semantic theory on indexical expressions claims that demonstratively used indexicals such as this lack a referent-determining meaning but instead rely on an accompanying demonstration act like a pointing gesture. While this view allows to set up a sound logic of demonstratives, the direct-referential role assigned to pointing gestures has never been scrutinized thoroughly in semantics or pragmatics. We investigate the semantics and pragmatics of co-verbal pointing from a foundational perspective combining experiments, statistical investigation, computer simulation and theoretical modeling techniques in a novel manner. We evaluate various referential hypotheses with a corpus of object identification games set up in experiments in which body movement tracking techniques have been extensively used to generate precise pointing measurements. Statistical investigation and computer simulations show that especially distal areas in the pointing domain falsify the semantic direct-referential hypotheses concerning pointing gestures. As an alternative, we propose that reference involving pointing rests on a default inference which we specify using the empirical data. These results raise numerous problems for classical semantics–pragmatics interfaces: we argue for pre-semantic pragmatics in order to account for inferential reference in addition to classical post-semantic Gricean pragmatics.},
  author       = {Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes},
  journal      = {Journal of Pragmatics},
  keyword      = {Multimodal Communication
Gestures
Pointing
Reference},
  pages        = {56--79},
  publisher    = {Elsevier},
  title        = {{Pointing and reference reconsidered}},
  doi          = {10.1016/j.pragma.2014.12.013},
  volume       = {77},
  year         = {2015},
}

@article{12,
  abstract     = {Eye tracking helps evaluate the quality of data visualization techniques and facilitates advanced interaction techniques for visualization
systems.},
  author       = {Kurzhals, Kuno and Burch, Michael and Pfeiffer, Thies and Weiskopf, Daniel},
  journal      = {Computing in Science \& Engineering},
  keyword      = {Eyetracking},
  number       = {5},
  pages        = {64--71},
  publisher    = {IEEE},
  title        = {{Eye Tracking in Computer-Based Visualization}},
  doi          = {10.1109/MCSE.2015.93},
  volume       = {17},
  year         = {2015},
}

@inproceedings{13,
  abstract     = {Die Erkennung und Verfolgung von Objekten ist seit Jahrzehnten eine wichtige Basis für Anwendungen im Bereich der Erweiterten Realität. Muss aus der Vielzahl an Bibliotheken eine verfügbare ausgewählt werden, fehlt es häufig an Vergleichbarkeit, da es kein standardisiertes Testverfahren gibt. Gleichzeitig ist es für die Entwicklung
eigener Verfahren essentiell, das Optimierungspotential genau bestimmen zu können. Die vorliegende Arbeit versucht diese Lücke zu füllen: Mithilfe von systematisch erstellten gerenderten Videos können verschiedene Verfahren und Bibliotheken bezüglich ihrer Genauigkeit
und Performanz überprüft werden. Exemplarisch werden die Eigenschaften dreier Trackingbibliotheken miteinander verglichen.},
  author       = {Diekmann, Jonas and Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 12. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Hinkenjann, André and Maiero, Jens and Blach, Roland},
  location     = {Bonn},
  pages        = {89--100},
  publisher    = {Shaker Verlag},
  title        = {{Framework zur Evaluation von Trackingbibliotheken mittels gerenderter Videos von Tracking-Targets}},
  year         = {2015},
}

@inproceedings{14,
  abstract     = {In diesem Paper wird die Entwicklung und Evaluation eines kostengünstigen interaktiven Aufprojektionssystems auf Basis einer Microsoft Kinect 2 beschrieben. Das Aufprojektionssystem nutzt einen LED-Projektor zur Darstellung von Informationen auf einer ebenen Projektionsfläche. Durch eine Analyse von Infrarot- und Tiefenbild werden Fingerbewegungen erkannt und als Multi-Touch-Events auf Windows-Betriebssystemebene bereitgestellt.
Die Tragfähigkeit des Ansatzes wird bezogen auf die erreichbare Genaugkeit und die Nutzbarkeit in einer Nutzerstudie evaluiert. Abschließend werden Designempfehlungen für die Gestaltung von Benutzerschnittstellen mit dem entwickelten interaktiven Aufprojektionssystem formuliert.},
  author       = {Neumann, Henri  and Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 12. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Hinkenjann, André and Maiero, Jens and Blach, Roland},
  location     = {Bonn},
  pages        = {22--33},
  publisher    = {Shaker Verlag},
  title        = {{Entwicklung und Evaluation eines Kinect v2-basierten Low-Cost Aufprojektionssystems}},
  year         = {2015},
}

@inproceedings{15,
  abstract     = {Measuring visual attention has become an important tool during product development. Attention maps are important qualitative visualizations to communicate results within the team and to stakeholders. We have developed a GPU-accelerated approach which allows for real-time generation of attention maps for 3D models that can, e.g., be used for on-the-fly visualizations of visual attention
distributions and for the generation of heat-map textures for offline high-quality renderings. The presented approach is unique in that it works with monocular and binocular data, respects the depth of focus, can handle moving objects and is ready to be used for selective
rendering.},
  author       = {Pfeiffer, Thies and Memili, Cem},
  booktitle    = {Proceedings of the IEEE VR 2015},
  editor       = {Höllerer, Tobias and Interrante, Victoria and Lécuyer, Anatole and II, J. Edward Swan},
  keyword      = {Attention Volumes, Gaze-based Interaction, 3D, Eye Tracking},
  pages        = {257--258},
  publisher    = {IEEE},
  title        = {{GPU-accelerated Attention Map Generation for Dynamic 3D Scenes}},
  year         = {2015},
}

@inbook{16,
  abstract     = {We present research-in-progress on an attentive in-store mobile recommender system that is integrated into the user’s glasses and worn during purchase decisions. The system makes use of the Attentive Mobile Interactive Cognitive Assistant (AMICA) platform prototype designed as a ubiquitous technology that supports people in their everyday-life. This paper gives a short overview of the technology and presents results from a pre-study in which we collected real-life eye-tracking data during decision processes in a supermarket. The data helps us to characterize and identify the different decision contexts based on differences in the observed attentional processes. AMICA provides eye-tracking data that can be used to classify decision-making behavior in real-time to make a recommendation process context-aware.},
  author       = {Pfeiffer, Jella and Pfeiffer, Thies and Meißner, Martin},
  booktitle    = {Reshaping Society through Analytics, Collaboration, and Decision Support},
  editor       = {Power, Daniel and Lakshmi, Iyer},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  pages        = {161--173},
  publisher    = {Springer International Publishing},
  title        = {{Towards attentive in-store recommender systems}},
  volume       = {18},
  year         = {2015},
}

@article{17,
  abstract     = {Human-robot interaction in shared spaces might benefit from human skills of anticipating movements. We observed human-human interactions in a route planning scenario to   identify relevant communication strategies with a focus on hand-eye coordination.},
  author       = {Renner, Patrick and Pfeiffer, Thies and Wachsmuth, Sven},
  journal      = {Cognitive Processing},
  keyword      = {gestures
eye tracking
robotics
},
  number       = {1 Supplement},
  pages        = {59--60},
  title        = {{Towards a model for anticipating human gestures in human-robot interactions in shared space}},
  volume       = {15},
  year         = {2014},
}

@article{18,
  abstract     = {We  present  an approach to identify  the 3D point  of regard and the fixated object in real-time based on 2D gaze videos without  the  need for manual annotation. The approach does not require  additional hardware except for the mobile eye tracker. It is currently applicable for scenarios with static target objects and requires an instrumentation of the environment with markers. The system has already been tested in two different studies. Possible applications are visual world paradigms in complex 3D environments, research on visual attention or human-human/human-agent interaction studies.},
  author       = {Pfeiffer, Thies and Renner, Patrick and Pfeiffer-Leßmann, Nadine},
  journal      = {Cognitive Processing},
  keyword      = {Gaze-based Interaction
Eye Tracking},
  number       = {Suppl. 1},
  pages        = {S127--S129},
  publisher    = {Springer},
  title        = {{Efficient analysis of gaze-behavior in 3D environments}},
  volume       = {15},
  year         = {2014},
}

@inbook{19,
  abstract     = {For solving tasks cooperatively in close interaction with humans, robots need to have timely updated spatial representations. However, perceptual information about the current position of interaction partners is often late. If robots could anticipate the targets of upcoming manual actions, such as pointing gestures, they would have more time to physically react to human movements and could consider prospective space allocations in their planning. Many findings support a close eye-hand coordination in humans which could be used to predict gestures by observing eye gaze. However, effects vary strongly with the context of the interaction. We collect evidence of eye-hand coordination in a natural route planning scenario in which two agents interact over a map on a table. In particular, we are interested if fixations can predict pointing targets and how target distances affect the interlocutor's pointing behavior. We present an automatic method combining marker tracking and 3D modeling that provides eye and gesture measurements in real-time.},
  author       = {Renner, Patrick and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Spatial Cognition IX},
  editor       = {Freksa, Christian and Nebel, Bernhard and Hegarty, Mary and Barkowsky, Thomas},
  keyword      = {gestures, robotics, eye tracking, multimodal interaction},
  pages        = {121--136},
  publisher    = {Springer},
  title        = {{Spatial references with gaze and pointing in shared space of humans and robots}},
  doi          = {10.1007/978-3-319-11215-2_9},
  volume       = {8684},
  year         = {2014},
}

@inproceedings{20,
  abstract     = {For solving complex tasks cooperatively in close interaction with robots, they need to understand natural human communication. To achieve this, robots could benefit from a deeper understanding of the processes that humans use for successful communication. Such skills can be studied by investigating human face-to-face interactions in complex tasks. In our work the focus lies on shared-space interactions in a path planning task and thus 3D gaze directions and hand movements are of particular interest. However, the analysis of gaze and gestures is a time-consuming task: Usually, manual annotation of the eye tracker's scene camera video is necessary in a frame-by-frame manner. To tackle this issue, based on the EyeSee3D method, an automatic approach for annotating interactions is presented: A combination of geometric modeling and 3D marker tracking serves to align real world stimuli with virtual proxies. This is done based on the scene camera images of the mobile eye tracker alone. In addition to the EyeSee3D approach, face detection is used to automatically detect fixations on the interlocutor. For the acquisition of the gestures, an optical marker tracking system is integrated and fused in the multimodal representation of the communicative situation.},
  author       = {Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  keyword      = {Eyetracking, geometric modelling, motion tracking, Gaze-based Interaction, 3D gaze analysis, Augmented Reality, eye tracking, marker tracking},
  pages        = {361--362},
  publisher    = {ACM},
  title        = {{Model-based acquisition and analysis of multimodal interactions for improving human-robot interaction}},
  doi          = {10.1145/2578153.2582176},
  year         = {2014},
}

@inproceedings{21,
  abstract     = {For validly analyzing human visual attention, it is often necessary to proceed from computer-based desktop set-ups to more natural real-world settings. However, the resulting loss of control has to be counterbalanced by increasing 
participant and/or item count. Together with the effort required to manually annotate the gaze-cursor videos recorded with mobile eye trackers, this renders many studies unfeasible.

We tackle this issue by minimizing the need for manual annotation of mobile gaze data. Our approach combines geo\-metric modelling with inexpensive 3D marker tracking to align virtual proxies with the real-world objects. This allows us to classify fixations on objects of interest automatically while supporting a completely free moving participant.

The paper presents the EyeSee3D method as well as a comparison of an expensive outside-in (external cameras) and a low-cost inside-out (scene camera) tracking of the eyetracker's position. The EyeSee3D approach is evaluated comparing the results from automatic and manual classification of fixation targets, which raises old problems of annotation validity in a modern context.},
  author       = {Pfeiffer, Thies and Renner, Patrick},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  keyword      = {Gaze-based Interaction
Eyetracking
Augmented Reality
},
  pages        = {195--202},
  publisher    = {ACM},
  title        = {{EyeSee3D: a low-cost approach for analysing mobile 3D eye tracking data using augmented reality technology}},
  doi          = {10.1145/2578153.2578183},
  year         = {2014},
}

@inproceedings{22,
  abstract     = {In this paper, we investigate the visual attention of consumers with the help of mobile eye-tracking technology. We explore attentional differences between goal-directed search and exploratory search used when consumers are purchasing a product at the point-of-sale. The aim of this study is to classify these two search processes based solely on the consumersâ eye movements. Using data from a field experiment in a supermarket, we build a model that learns about consumersâ attentional processes and makes predictions about the search process they used. Our results show that we can correctly classify the search processes used with an accuracy of nearly 70 after just the first nine seconds of the search. Later on in the search process, the accuracy of the classification can reach up to 77.},
  author       = {Pfeiffer, Jella and Meißner, Martin and Prosiegel, Jascha and Pfeiffer, Thies},
  booktitle    = {Proceedings of the International Conference on Information Systems 2014 (ICIS 2014)},
  keyword      = {Recommendation Systems, Decision Support Systems (DSS), Human Information Behavior, Neuro-IS, Mobile commerce},
  title        = {{Classification of Goal-Directed Search and Exploratory Search Using Mobile Eye-Tracking}},
  year         = {2014},
}

@inproceedings{23,
  abstract     = {Previous work on eye tracking and eye-based
human-computer interfaces mainly concentrated on
making use of the eyes in traditional desktop settings.
With the recent growth of interest in smart glass devices
and low-cost eye trackers, however, gaze-based techniques
for mobile computing is becoming increasingly important.
PETMEI 2014 focuses on the pervasive eye tracking
paradigm as a trailblazer for mobile eye-based interaction
and eye-based context-awareness. We want to stimulate
and explore the creativity of these communities with
respect to the implications, key research challenges, and
new applications for pervasive eye tracking in ubiquitous
computing. The long-term goal is to create a strong
interdisciplinary research community linking these fields
together and to establish the workshop as the premier
forum for research on pervasive eye tracking.},
  author       = {Pfeiffer, Thies and Stellmach, Sophie and Sugano, Yusuke},
  booktitle    = {UbiComp'14 Adjunct: The 2014 ACM Conference on Ubiquitous Computing Adjunct Publication},
  keyword      = {Gaze-based Interaction},
  pages        = {1085--1092},
  title        = {{4th International Workshop on Pervasive Eye Tracking and Mobile Eye-Based Interaction}},
  year         = {2014},
}

@inproceedings{24,
  abstract     = {In many applications it is necessary to guide humans' visual attention towards certain points in the environment. This can be to highlight certain attractions in a touristic application for smart glasses, to signal important events to the driver of a car or to draw the attention of a user of a desktop system to an important message of the user interface. The question we are addressing here is: How can we guide visual attention if we are not able to do it visually? In the presented approach we use gaze-contingent auditory feedback (sonification) to guide visual attention and show that people are able to make use of this guidance to speed up visual search tasks significantly.},
  author       = {Losing, Viktor and Rottkamp, Lukas and Zeunert, Michael and Pfeiffer, Thies},
  booktitle    = {UbiComp'14 Adjunct: The 2014 ACM Conference on Ubiquitous Computing Adjunct Publication},
  keyword      = {Gaze-based Interaction, Visual Search},
  location     = {Seattle, WA, USA},
  pages        = {1093--1102},
  publisher    = {ACM Press},
  title        = {{Guiding Visual Search Tasks Using Gaze-Contingent Auditory Feedback}},
  year         = {2014},
}

@inproceedings{25,
  abstract     = {Since the widespread diffusion of mobile devices, like smartphones, mobile decision support systems (MDSS) that provide product information, recommendations or other kind of decision support for in-store purchases have gained momentum. A user-centered design of MDSS requires a choice of features appropriate for the specific decision situation. This paper presents results of a study to identify important features customers expect of an in-store MDSS for electronic devices in different purchase situations. The study has been conducted as online questionnaire applying a preference measurement technique from marketing science.},
  author       = {Huschens, Martin and Pfeiffer, Jella and Pfeiffer, Thies},
  booktitle    = {Proceedings of the MKWI},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  title        = {{Important product features of mobile decision support systems for in-store purchase decisions: A user-perspective taking into account different purchase situations}},
  year         = {2014},
}

@inproceedings{26,
  abstract     = {One of the latest trends of ubiquitous Information Systems is the use of smartglasses, such as Google Glass or Epson Moverio BT-200 that are connected to the Internet and are augmenting reality with a head-up display. In order to develop recommendation agents (RAs) for the use at the point of sale, researchers have proposed to integrate a portable eye tracking system into such smartglasses (Pfeiffer et al. 2013). This would allow providing the customer with relevant product information and alternative products by making use of the customer’s information acquisition processes recorded during the purchase decision.},
  author       = {Pfeiffer, Jella and Prosiegel, Jascha and Meißner, Martin and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Gmunden Retreat on NeuroIS 2014},
  editor       = {Davis, Fred and Riedl, René and vom Brocke, Jan and Léger, Pierre-Majorique and Randolph, Adriane},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  location     = {Gmunden, Austria},
  pages        = {23--25},
  title        = {{Identifying goal-oriented and explorative information search patterns}},
  year         = {2014},
}

@inproceedings{27,
  author       = {Kousidis, Spyridon and Pfeiffer, Thies and Schlangen, David},
  booktitle    = {Proceedings of Interspeech 2013},
  keyword      = {Multimodal Communication},
  location     = {Lyon, France},
  publisher    = {ISCA},
  title        = {{MINT.tools: Tools and Adaptors Supporting Acquisition, Annotation and Analysis of Multimodal Corpora}},
  year         = {2013},
}

@inbook{28,
  abstract     = {The eyes play an important role both in perception and communication. Technical interfaces that make use of their versatility can bring significant improvements to those who are unable to speak or to handle selection tasks elsewise such as with their hands, feet, noses or tools handled with the mouth. Using the eyes to enter texts into a computer system, which is called gaze-typing, is the most prominent gaze-based assistive technology. The article reviews the principles of eye movements, presents an overview of current eye-tracking systems, and discusses several approaches to gaze-typing. With the recent advent of mobile eye-tracking systems, gaze-based assistive technology is no longer restricted to interactions with desktop-computers. Gaze-based assistive technology is ready to expand its application into other areas of everyday life. The second part of the article thus discusses the use of gaze-based assistive technology in the household, or “the wild,” outside one’s own four walls.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Assistive Technologies and Computer Access for Motor Disabilities},
  editor       = {Kouroupetroglou, Georgios},
  keyword      = {Gaze-based Interaction
Eye Tracking},
  pages        = {90--109},
  publisher    = {IGI Global},
  title        = {{Gaze-based assistive technologies}},
  doi          = {10.4018/978-1-4666-4438-0.ch004},
  year         = {2013},
}

@misc{29,
  author       = {Pfeiffer, Thies and Essig, Kai},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  editor       = {Holmqvist, Kenneth and Mulvey, F. and Johansson, Roger},
  keyword      = {Gaze-based Interaction, Eyetracking},
  location     = {Lund, Sweden},
  number       = {3},
  pages        = {275--275},
  publisher    = {Journal of Eye Movement Research},
  title        = {{Analysis of eye movements in situated natural interactions}},
  volume       = {6},
  year         = {2013},
}

@inproceedings{30,
  abstract     = {We aim to utilize online information about visual attention for developing mobile recommendation agents (RAs) for use at the point of sale. Up to now, most RAs are focussed exclusively at personalization in an e-commerce setting. Very little is known, however, about mobile RAs that offer information and assistance at the point of sale based on individual-level feature based preference models (Murray and Häubl 2009). Current attempts provide information about products at the point of sale by manually scanning barcodes or using RFID (Kowatsch et al. 2011, Heijden 2005), e.g. using specific apps for smartphones. We argue that an online access to the current visual attention of the user offers a much larger potential. Integrating mobile eye tracking into ordinary glasses would yield a direct benefit of applying neuroscience methods in the user’s everyday life. First, learning from consumers’ attentional processes over time and adapting recommendations based on this learning allows us to provide very accurate and relevant recommendations, potentially increasing the perceived usefulness. Second, our proposed system needs little explicit user input (no scanning or navigation on screen) making it easy to use. Thus, instead of learning from click behaviour and past customer ratings, as it is the case in the e-commerce setting, the mobile RA learns from eye movements by participating online in every day decision processes. We argue that mobile RAs should be built based on current research in human judgment and decision making (Murray et al. 2010). In our project, we therefore follow a two-step approach: In the empirical basic research stream, we aim to understand the user’s interaction with the product shelf: the actions and patterns of user’s behaviour (eye movements, gestures, approaching a product closer) and their correspondence to the user’s informational needs. In the empirical system development stream, we create prototypes of mobile RAs and test experimentally the factors that influence the user’s adoption. For example, we suggest that a user’s involvement in the process, such as a need for exact nutritional information or for assistance (e.g., reading support for elderly) will influence the user’s intention to use such as system. The experiments are conducted both in our immersive virtual reality supermarket presented in a CAVE, where we can also easily display information to the user and track the eye movement in great accuracy, as well as in real-world supermarkets (see Figure 1), so that the findings can be better generalized to natural decision situations (Gidlöf et al. 2013). In a first pilot study with five randomly chosen participants in a supermarket, we evaluated which sort of mobile RAs consumers favour in order to get a first impression of the user’s acceptance of the technology. Figure 1 shows an excerpt of one consumer’s eye movements during a decision process. First results show long eye cascades and short fixations on many products in situations where users are uncertain and in need for support. Furthermore, we find a surprising acceptance of the technology itself throughout all ages (23 – 61 years). At the same time, consumers express serious fear of being manipulated by such a technology. For that reason, they strongly prefer the information to be provided by trusted third party or shared with family members and friends (see also Murray and Häubl 2009). Our pilot will be followed by a larger field experiment in March in order to learn more about factors that influence the user’s acceptance as well as the eye movement patterns that reflect typical phases of decision processes and indicate the need for support by a RA.},
  author       = {Pfeiffer, Thies and Pfeiffer, Jella and Meißner, Martin},
  booktitle    = {Proceedings of the Gmunden Retreat on NeuroIS 2013},
  editor       = {Davis, Fred and Riedl, René and Jan, vom Brocke and Léger, Pierre-Majorique and Randolph, Adriane},
  keyword      = {Mobile Cognitive Assistance Systems
Information Systems},
  location     = {Gmunden},
  pages        = {3--3},
  title        = {{Mobile recommendation agents making online use of visual attention information at the point of sale}},
  year         = {2013},
}

@misc{31,
  author       = {Pfeiffer, Thies and Essig, Kai},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  pages        = {275},
  publisher    = {Journal of Eye Movement Research},
  title        = {{Analysis of eye movements in situated natural interactions}},
  year         = {2013},
}

@inproceedings{32,
  abstract     = {The provision of stereo images to facilitate depth perception by stereopsis is one key aspect of many Virtual Reality installations and there are many technical approaches to do so. However, differences in visual capabilities of the user and technical limitations of a specific set-up might restrict the spatial range in which stereopsis can be facilitated. In this paper, we transfer an existent test for stereo vision from the real world to a virtual environment and extend it to measure stereo acuity.},
  author       = {Dankert, Timo and Heil, Dimitri and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 10. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Staadt, Oliver and Steinicke, Frank},
  keyword      = {Virtual Reality},
  pages        = {185--188},
  publisher    = {Shaker Verlag},
  title        = {{Stereo vision and acuity tests within a virtual reality set-up}},
  year         = {2013},
}

@misc{33,
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction},
  location     = {Bielefeld},
  publisher    = {CITEC},
  title        = {{Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)}},
  doi          = {10.2390/biecoll-saga2013_0},
  year         = {2013},
}

@inproceedings{34,
  abstract     = {Modern mobile eye trackers calculate the point-of-regard relatively to the current image obtained by a scene-camera. They show where the wearer of the eye tracker is looking at in this 2D picture, but they fail to provide a link to the object of interest in the environment. To understand the context of the wearer’s current actions, human annotators therefore have to label the recorded fixations manually. This is very time consuming and also prevents an online interactive use in HCI. A popular scenario for mobile eye tracking are supermarkets. Gidlöf et al. (2013) used this scenario to study the visual behaviour in a decision-process. De Beugher et al. (2012) developed an offline approach to automate the analysis of object identification. For usage of mobile eye tracking in an online recommender system (Pfeiffer et al., 2013), that supports the user in a supermarket, it is essential to identify the object of interest immediately. Our work addresses this issue by using location information to speed-up the identification of the fixated object and at the same time making detection results more robust.},
  author       = {Harmening, Kai and Pfeiffer, Thies},
  booktitle    = {Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction
Mobile Cognitive Assistance Systems},
  location     = {Bielefeld},
  pages        = {38--40},
  publisher    = {Center of Excellence Cognitive Interaction Technology},
  title        = {{Location-based online identification of objects in the centre of visual attention using eye tracking}},
  doi          = {10.2390/biecoll-saga2013_10},
  year         = {2013},
}

@inproceedings{35,
  abstract     = {In a typical grocery-shopping trip consumers are overwhelmed not only by the number of products and brands in the store, but also by other possible distractions like advertisements, other consumers or smartphones. In this environment, attention is the key source for investigating the decision processes of customers. Recent mobile eyetracking systems have opened the gate to a better understanding of instore attention. We present perspectives from the two disciplines marketing research and human-computer interaction and refine methodical and technological requirements for attention analysis at the point-of-sale (POS).},
  author       = {Meißner, Martin and Pfeiffer, Jella and Pfeiffer, Thies},
  booktitle    = {Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction
Mobile Cognitive Assistance Systems},
  location     = {Bielefeld},
  pages        = {10--13},
  publisher    = {Center of Excellence Cognitive Interaction Technology},
  title        = {{Mobile eyetracking for decision analysis at the point-of-sale: Requirements from the perspectives of marketing research and human-computer interaction}},
  doi          = {10.2390/biecoll-saga2013_3},
  year         = {2013},
}

@inproceedings{36,
  author       = {Essig, Kai and Pfeiffer, Thies and Maycock, Jonathan and Schack, Thomas},
  booktitle    = {ISSP 13th World Congress of Sport Psychology - Harmony and Excellence in Sport and Life},
  editor       = {Chi, J.},
  location     = {Bejing Sports University, Bejing, China},
  pages        = {43--46},
  title        = {{Attentive Systems: Modern Analysis Techniques for Gaze Movements in Sport Science}},
  year         = {2013},
}

@inproceedings{37,
  abstract     = {If robots are to successfully interact in a space shared with humans, they should learn the communicative signals humans use in face-to-face interactions. For example, a robot can consider human presence for grasping decisions using a representation of peripersonal space (Holthaus \&Wachsmuth, 2012). During interaction, the eye gaze of the interlocutor plays an important role. Using mechanisms of joint attention, gaze can be used to ground objects during interaction and knowledge about the current goals of the interlocutor are revealed (Imai et al., 2003). Eye movements are also known to precede hand pointing or grasping (Prablanc et al., 1979), which could help robots to predict areas with human activities, e.g. for security reasons. We aim to study patterns of gaze and pointing in interaction space. The human participants’ task is to jointly plan routes on a floor plan. For analysis, it is necessary to find fixations on specific rooms and floors as well as on the interlocutor’s face or hands. Therefore, a model-based approach for automating this mapping was developed. This approach was evaluated using a highly accurate outside-in tracking system as baseline and a newly developed low-cost inside-out marker-based tracking system making use of the eye tracker’s scene camera.},
  author       = {Renner, Patrick and Pfeiffer, Thies},
  booktitle    = {Proceedings of the First International Workshop on Solutions for Automatic Gaze-Data Analysis 2013 (SAGA 2013)},
  editor       = {Pfeiffer, Thies and Essig, Kai},
  keyword      = {Gaze-based Interaction},
  location     = {Bielefeld},
  pages        = {28--31},
  publisher    = {Center of Excellence Cognitive Interaction Technology},
  title        = {{Studying joint attention and hand-eye coordination in human-human interaction: A model-based approach to an automatic mapping of fixations to target objects}},
  doi          = {10.2390/biecoll-saga2013_7},
  year         = {2013},
}

@inproceedings{38,
  abstract     = {In this paper we evaluate spatial presence and orientation in the OCTAVIS system, a novel virtual reality platform aimed at training and rehabilitation of visual-spatial cognitive abilities. It consists of eight touch-screen displays surrounding the user, thereby providing a 360 horizontal panorama view. A rotating office chair and a joystick in the armrest serve as input devices to easily navigate through the virtual environment. We conducted a two-step experiment to investigate spatial orientation capabilities with our device. First, we examined whether the extension of the horizontal field of view from 135 (three displays) to 360 (eight displays) has an effect on spatial presence and on the accuracy in a pointing task. Second, driving the full eight screens, we explored the effect of embodied self-rotation using the same measures. In particular we compare navigation by rotating the world while the user is sitting stable to a stable world and a self-rotating user.},
  author       = {Dyck, Eugen and Pfeiffer, Thies and Botsch, Mario},
  booktitle    = {Joint Virtual Reality Conference of EGVE - EuroVR},
  editor       = {Mohler, Betty and Raffin, Bruno and Saito, Hideo and Staadt, Oliver},
  keyword      = {Virtual Reality},
  pages        = {1--8},
  publisher    = {Eurographics Association},
  title        = {{Evaluation of Surround-View and Self-Rotation in the OCTAVIS VR-System}},
  doi          = {10.2312/EGVE.JVRC13.001-008},
  year         = {2013},
}

@article{39,
  abstract     = {Unsere Augen sind für die Wahrnehmung unserer Umwelt wichtig und geben gleichzeitig wertvolle Informationen über unsere Aufmerksamkeit und damit unsere Denkprozesse preis. Wir Menschen nutzen dies ganz natürlich in der alltäglichen Kommunikation. Mit einer echtzeitfähigen Blickbewegungsmessung ausgestattet können auch technische Systeme den Nutzern wichtige Informationen von den Augen ablesen. Der Artikel beschreibt verschiedene Ansätze wie in der Konstruktion, der Instruktion von Robotern oder der Medizin Blickbewegungen nutzbringend eingesetzt werden können. 
/
We use our eyes to perceive our everyday environment. In doing so, we also reveal our current focus of attention and thus allow others to draw insights regarding our internal cognition processes. We humans make use of this dual function of the eyes quite naturally in everyday communication. Using realtime eye tracking, technical systems can learn to read relevant information from their users' eyes. This article describes approaches to make use of gaze information in construction tasks, the instruction of robots or in medical applications.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  journal      = {at - Automatisierungstechnik},
  keyword      = {Visual Attention, Multimodal Interaction, Human-Machine Interaction, Eye Tracking, Attentive Interfaces, Visuelle Aufmerksamkeit, Multimodale Interaktion, Mensch-Maschine-Interaktion, Aufmerksame Benutzerschnittstellen, Blickbewegungsmessung, Gaze-based Interaction},
  number       = {11},
  pages        = {770--776},
  publisher    = {Walter de Gruyter GmbH},
  title        = {{Multimodale blickbasierte Interaktion}},
  doi          = {10.1524/auto.2013.1058},
  volume       = {61},
  year         = {2013},
}

@inproceedings{40,
  abstract     = {Das Paper arbeitet den Forschungsstand zur Überwindung von Höhenunterschieden in der Virtuellen Realität (VR) auf und diskutiert insbesondere deren Einsatz in egozentrischer Perspektive. Am konkreten Beispiel einer VR-Version des Computerspiels Minecraft wird herausgestellt, dass bestehende Ansätze den Anforderungen dieser Anwendungen nicht genügen.},
  author       = {Orlikowski, Matthias and Bongartz, Richard and Reddersen, Andrea and Reuter, Jana and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - 10. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Staadt, Oliver and Steinicke, Frank},
  keyword      = {Virtual Reality, Human-Computer Interaction},
  pages        = {193--196},
  publisher    = {Shaker Verlag},
  title        = {{Springen in der Virtuellen Realität: Analyse von Navigationsformen zur Überwindung von Höhenunterschieden am Beispiel von MinecraftVR}},
  year         = {2013},
}

@inproceedings{41,
  abstract     = {Die synthetische Stimulation der visuellen Wahrnehmung ist seit jeher im Fokus von Virtueller und Erweiterter Realität und die möglichst exakte Bestimmung der Nutzerperspektive auf die dreidimensionale Welt eine der Kernaufgaben. Bislang gibt es jedoch nur einige exemplarische Ansätze, in denen die Blickrichtung des Nutzers oder gar die Verteilung der visuellen Aufmerksamkeit im Raum genauer bestimmt wird. Macht man diese Informationen der Anwendungslogik verfügbar, könnten existierende Verfahren zur Visualisierung weiter optimiert und neue Verfahren entwickelt werden. Darüber hinaus erschließen sich damit Blicke als Interaktionsmodalität. Aufbauend auf langjährigen Erfahrungen mit der Blickinteraktion in der Virtuellen Realität beschreibt der Artikel Komponenten für einen Szenengraph, mit dem sich blickbasierte Interaktionen leicht und entlang gewohnter Prinzipien realisieren lassen.},
  author       = {Pfeiffer, Thies},
  booktitle    = {11. Paderborner Workshop Augmented and Virtual Reality in der Produktentstehung},
  editor       = {Gausemeier, Jürgen and Grafe, Michael and Meyer auf der Heide, Friedhelm},
  keyword      = {Virtual Reality, Human-Machine Interaction, Visual Attention, Gaze-based Interaction},
  pages        = {295--307},
  publisher    = {Heinz Nixdorf Institut, Universität Paderborn},
  title        = {{Visuelle Aufmerksamkeit in Virtueller und Erweiterter Realität: Integration und Nutzung im Szenengraphen}},
  volume       = {311},
  year         = {2013},
}

@inbook{42,
  abstract     = {Human hand gestures are very swift and difficult to observe from the (often) distant perspective of a scientific overhearer. Not uncommonly, fingers are occluded by other body parts or context objects and the true hand posture is often only revealed to the addressee. In addition to that, as the hand has many degrees of freedom and the annotation has to cover positions and orientations in a 3D world – which is less accessible from the typical computer-desktop workplace of an annotator than, let’s say, spoken language – the annotation of hand postures is quite expensive and complex.

Fortunately, the research on virtual reality technology has brought about data gloves in the first place, which were meant as an interaction device allowing humans to manipulate entities in a virtual world. Since its release, however, many different applications have been found. Data gloves are devices that track most of the joints of the human hand and generate data-sets describing the posture of the hand several times a second. The article reviews different types of data gloves, discusses representation formats and ways to support annotation, and presents best practices for study design using data gloves as recording devices.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Body-Language-Communication: An International Handbook on Multimodality in Human Interaction},
  editor       = {Müller, Cornelia and Cienki, Alan and Fricke, Ellen and Ladewig, Silva H. and McNeill, David and Teßendorf, Sedinha},
  keyword      = {Multimodal Communication, Multimodal Corpora, Motion Capturing, Data Gloves},
  pages        = {868--869},
  publisher    = {Mouton de Gruyter},
  title        = {{Documentation of gestures with data gloves}},
  doi          = {10.1515/9783110261318.868},
  volume       = {38/1},
  year         = {2013},
}

@inbook{43,
  abstract     = {For the scientific observation of non-verbal communication behavior, video recordings are the state of the art. However, everyone who has conducted at least one video-based
study has probably made the experience, that it is difficult to get the setup right, with respect to image resolution, illumination, perspective, occlusions, etc. And even more effort is needed for the annotation of the data. Frequently, even short interaction sequences may consume weeks or even months of rigorous full-time annotations.
One way to overcome some of these issues is the use of motion capturing for assessing (not only) communicative body movements. There are several competing tracking technologies available, each with its own benefits and drawbacks. The article provides an overview of the basic types of tracking systems, presents representation formats and tools for the analysis of motion data, provides pointers to some studies using motion capture and discusses best practices for study design. However, the article also stresses that motion capturing still requires some expertise and is only starting to become mobile
and reasonably priced – arguments not to be neglected.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science},
  editor       = {Müller, Cornelia and Cienki, Alan and Fricke, Ellen and Ladewig, Silva H. and McNeill, David and Teßendorf, Sedinha},
  keyword      = {Multimodal Communication, Motion Capturing, Gesture Annotation, Multimodal Corpora},
  pages        = {857--868},
  publisher    = {Mouton de Gruyter},
  title        = {{Documentation of gestures with motion capture}},
  doi          = {10.1515/9783110261318.857},
  volume       = {38/1},
  year         = {2013},
}

@misc{44,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Book of Abstracts of the 17th European Conference on Eye Movements},
  editor       = {Holmqvist, Kenneth and Mulvey, F. and Johansson, Roger},
  keyword      = {Joint Attention},
  location     = {Lund, Sweden},
  number       = {3},
  pages        = {152--152},
  publisher    = {Journal of Eye Movement Research},
  title        = {{A model of joint attention for humans and machines}},
  volume       = {6},
  year         = {2013},
}

@inproceedings{45,
  abstract     = {A fundamental problem in manual based gesture semantics reconstruction is the specification of preferred semantic concepts for gesture trajectories. This issue is complicated by problems human raters have annotating fast-paced three dimensional trajectories. Based on a detailed example of a gesticulated circular trajectory,
we present a data-driven approach that covers parts of the semantic reconstruction by making use of motion capturing
(mocap) technology. In our FA3ME framework we use a complex event processing approach to analyse and annotate multi-modal events. This framework provides grounds for a detailed description of how to get at the semantic concept of circularity observed in the data.},
  author       = {Pfeiffer, Thies and Hofmann, Florian and Hahn, Florian and Rieser, Hannes and Röpke, Insa},
  booktitle    = {Proceedings of the Special Interest Group on Discourse and Dialog (SIGDIAL) 2013 Conference},
  editor       = {Eskenazi, Maxine and Strube, Michael and Di Eugenio, Barbara and Williams, Jason D.},
  keyword      = {Multimodal Communication},
  location     = {Metz, France},
  pages        = {270--279},
  publisher    = {Association for Computational Linguistics},
  title        = {{Gesture semantics reconstruction based on motion capturing and complex event processing: a circular shape example}},
  year         = {2013},
}

@inproceedings{46,
  abstract     = {This paper presents ongoing work on the design, deployment and evaluation of a multimodal data acquisition architecture
which utilises minimally invasive motion, head, eye and gaze tracking alongside high-quality audiovisual recording of
human interactions. The different data streams are centrally collected and visualised at a single point and in real time by
means of integration in a virtual reality (VR) environment. The overall aim of this endeavour is the implementation of a
multimodal data acquisition facility for the purpose of studying non-verbal phenomena such as feedback gestures,
hand and pointing gestures and multi-modal alignment. In the first part of this work that is described here, a series of tests
were performed in order to evaluate the feasibility of tracking feedback head gestures using the proposed architecture.},
  author       = {Kousidis, Spyridon and Pfeiffer, Thies and Malisz, Zofia and Wagner, Petra and Schlangen, David},
  booktitle    = {Proceedings of the Interdisciplinary Workshop on Feedback Behaviors in Dialog, INTERSPEECH2012 Satellite Workshop},
  keyword      = {Multimodal Communication},
  location     = {Stevenson, WA},
  pages        = {39--42},
  title        = {{Evaluating a minimally invasive laboratory architecture for recording multimodal conversational data.}},
  year         = {2012},
}

@misc{47,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of KogWis 2012},
  editor       = {Dörner, Dietrich and Goebel, Rainer and Oaksford, Mike and Pauen, Michael and Stern, Elsbeth},
  keyword      = {gaze-based interaction, cognitive modeling, joint attention},
  location     = {Bamberg, Germany},
  pages        = {96--97},
  publisher    = {University of Bamberg Press},
  title        = {{An operational model of joint attention - Timing of the initiate-act in interactions with a virtual human}},
  year         = {2012},
}

@inproceedings{48,
  author       = {Pfeiffer-Leßmann, Nadine and Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
  editor       = {Miyake, Naomi  and Peebles, David  and Cooper, Richard P. },
  keyword      = {joint attention, Gaze-based Interaction, social interaction, virtual humans},
  location     = {Sapporo, Japan},
  pages        = {851--856},
  publisher    = {Cognitive Science Society},
  title        = {{An operational model of joint attention - timing of gaze patterns in interactions between humans and a virtual human}},
  year         = {2012},
}

@inproceedings{49,
  abstract     = {Eye tracking and hand motion (or mouse) tracking are complementary techniques to study the dynamics underlying
human cognition. Eye tracking provides information about attention, reasoning, mental imagery, but figuring out the dynamics of cognition is hard. On the other hand, hand movement reveals the hidden states of high-level cognition as a continuous trajectory, but the detailed process is difficult to infer. Here, we use both eye and hand tracking while the subject watches a video drama and plays a multimodal memory game (MMG), a memory recall task designed to investigate the mechanism of recalling the contents of dramas. Our experimental results show that eye tracking and mouse tacking provide complementary information on cognitive processes. In particular, we found that, when humans make difficult decisions, they tend to ask ’Is the
distractor wrong?’, rather than ’Is the decision right?’.},
  author       = {Kim, Eun-Sol and Kim, Jiseob and Pfeiffer, Thies and Wachsmuth, Ipke and Zhang, Byoung-Tak},
  booktitle    = {Proceedings of the 34th Annual Meeting of the Cognitive Science Society},
  editor       = {Miyake, Naomi and Peebles, David and Cooper, Richard P.},
  keyword      = {Gaze-based Interaction},
  pages        = {2723},
  title        = {{‘Is this right?’ or ‘Is that wrong?’: Evidence from dynamic eye-hand movement in decision making [Abstract]}},
  year         = {2012},
}

@inproceedings{50,
  abstract     = {In this paper, we argue that empirical research on genuine linguistic topics, such as on the production of multimodal utterances in the speaker and the interpretation of the multimodal signals in the interlocutor, can greatly benefit from the use of virtual reality technologies. Established methodologies for research on multimodal interactions, such as the presentation of pre-recorded 2D videos of interaction partners as stimuli and the recording of interaction partners using multiple 2D video cameras have crucial shortcomings regarding ecological validity and the precision of measurements that can be achieved. In addition, these methodologies enforce restrictions on the researcher. The stimuli, for example, are not very interactive and thus not as close to natural interactions as ultimately desired. Also, the analysis of 2D video recordings requires intensive manual annotations, often frame-by-frame, which negatively affects the feasible number of interactions which can be included in a study. The technologies bundled under the term virtual reality offer exciting possibilities for the linguistic researcher: gestures can be tracked without being restricted to fixed perspectives, annotation can be done on large corpora (semi-)automatically and virtual characters can be used to produce specific linguistic stimuli in a repetitive but interactive fashion. Moreover, immersive 3D visualizations can be used to recreate a simulation of the recorded interactions by fusing the raw data with theoretic models to support an iterative data-driven development of linguistic theories. This paper discusses the potential of virtual reality technologies for linguistic research and provides examples for the application of the methodology.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Virtual Reality Short Papers and Posters (VRW)},
  editor       = {Coquillart, Sabine and Feiner, Steven and Kiyokawa, Kiyoshi},
  keyword      = {Linguistics, Motion Capturing, Intelligent Virtual Agents, Multimodal Communication, Virtual Reality},
  location     = {Costa Mesa, CA, USA},
  pages        = {83--84},
  publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
  title        = {{Using virtual reality technology in linguistic research}},
  doi          = {10.1109/VR.2012.6180893},
  year         = {2012},
}

@inproceedings{51,
  abstract     = {Modern computer-algebra programs are able to solve a wide
range of mathematical calculations. However, they are not able to understand and solve math text problems in which the equation is described in terms of natural language instead of mathematical formulas. Interestingly, there are only few known approaches to solve math word problems algorithmically and most of employ models based on frames. To overcome problems with existing models, we propose a model based on augmented semantic networks to represent the mathematical structure behind word problems. This model is implemented in our Solver for Mathematical Text Problems (SoMaTePs) [1], where the math problem is extracted via natural language processing, transformed in mathematical equations and solved by a state-of-the-art computer-algebra program. SoMaTePs is able to understand and solve mathematical text problems from German primary school books and could be extended to other languages by exchanging the language model in the natural language processing module.},
  author       = {Liguda, Christian and Pfeiffer, Thies},
  booktitle    = {Natural Language Processing and Information Systems/17th International Conference on Applications of Natural Language to Information Systems},
  editor       = {Bouma, Gosse and Ittoo, Ashwin and Métais, Elisabeth and Wortmann, Hans},
  keyword      = {Artificial Intelligence},
  location     = {Groningen, Netherlands},
  pages        = {247--252},
  publisher    = {Springer},
  title        = {{Modeling math word problems with augmented semantic networks}},
  doi          = {10.1007/978-3-642-31178-9_29},
  volume       = {7337},
  year         = {2012},
}

@inproceedings{52,
  abstract     = {Knowledge about the point of regard is a major key for the analysis of visual attention in areas such as psycholinguistics, psychology, neurobiology, computer science and human factors. Eye tracking is thus an established methodology in these areas, e.g. for investigating search processes, human communication behavior, product design or human-computer interaction. As eye tracking is a process which depends heavily on technology, the progress of gaze use in these scientific areas is tied to the advancements of eye-tracking technology. It is thus not surprising that in the last decades, research was primarily based on 2D stimuli and rather static scenarios, regarding both content and observer. Only with the advancements in mobile and robust eye-tracking systems, the observer is freed to physically interact in a 3D target scenario. Measuring and analyzing the point of regards in 3D space, however, requires additional techniques for data acquisition and scientific visualization. We describe the process for measuring the 3D point of regard and provide our own implementation of this process, which extends recent approaches of combining eye tracking with motion capturing, including holistic estimations of the 3D point of regard. In addition, we present a refined version of 3D attention volumes for representing and visualizing attention in 3D space.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the Symposium on Eye Tracking Research and Applications},
  editor       = {Spencer, Stephen N.},
  keyword      = {gaze tracking, visualization, motion tracking, Gaze-based Interaction, visual attention, 3d},
  location     = {Santa Barbara, CA, USA},
  pages        = {29--36},
  publisher    = {Association for Computing Machinery (ACM)},
  title        = {{Measuring and visualizing attention in space with 3D attention volumes}},
  doi          = {10.1145/2168556.2168560},
  year         = {2012},
}

@inproceedings{53,
  abstract     = {Referring to objects using multimodal deictic expressions is an important form of communication. This work addresses the question on how pragmatic factors affect content distribution between the modalities speech and gesture. This is done by analyzing a study on deictic pointing gestures to objects under two conditions: with and without speech. The relevant pragmatic factor was the distance to the referent object. As one main result two strategies were identified which were used by participants to adapt their gestures to the condition. This knowledge can be used, e.g., to improve the naturalness of pointing gestures employed by embodied conversational agents.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Gestures and Sign Language in Human-Computer Interaction and Embodied Communication, 9th International Gesture Workshop, GW 2011},
  editor       = {Efthimiou, Eleni and Kouroupetroglou, Georgios and Fotinea, Stavroula-Evita},
  keyword      = {Multimodal Communication},
  location     = {Athens, Greece},
  pages        = {238--249},
  publisher    = {Springer-Verlag GmbH},
  title        = {{Interaction between Speech and Gesture: Strategies for Pointing to Distant Objects}},
  doi          = {10.1007/978-3-642-34182-3_22},
  year         = {2012},
}

@inproceedings{54,
  abstract     = {The time course and the distribution of visual attention are powerful measures for the evaluation of the usability of products. Eye tracking is thus an established method for evaluating websites, software ergonomy or modern cockpits for cars or airplanes. In most cases, however, the point of regard is measured on 2D products. This article presents work that uses an approach to measure the point of regard in 3D to generate 3D Attention Volumes as a qualitative 3D visualization of the distribution of visual attention. This visualization can be used to evaluate the design of virtual products in an immersive 3D setting, similar as heatmaps are used to assess the design of websites.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2012},
  keyword      = {Gaze-based Interaction},
  location     = {Orange County, CA, USA},
  pages        = {117--118},
  publisher    = {IEEE},
  title        = {{3D Attention Volumes for Usability Studies in Virtual Reality}},
  year         = {2012},
}

@inproceedings{55,
  abstract     = {Swiftness and robustness of natural communication is tied to the redundancy and complementarity found in our multimodal communication. Swiftness and robustness of human-computer interaction (HCI) is also a key to the success of a virtual reality (VR) environment. The interpretation of multimodal interaction signals has therefore been considered a high goal in VR research, e.g. following the visions of Bolt's put-that-there in 1980. It is our impression that research on user interfaces for VR systems has been focused primarily on finding and evaluating technical solutions and thus followed a technology-oriented approach to HCI. In this article, we argue to complement this by a human-oriented approach based on the observation of human-human interaction. The aim is to find models of human-human interaction that can be used to create user interfaces that feel natural. As the field of Linguistics is dedicated to the observation and modeling of human-human communication, it could be worthwhile to approach natural user interfaces from a linguistic perspective. We expect at least two benefits from following this approach. First, the human-oriented approach substantiates our understanding of natural human interactions. Second, it brings about a new perspective by taking the interaction capabilities of a human addressee into account, which are not often explicitly considered or compared with that of the system. As a consequence of following both approaches to create user interfaces, we expect more general models of human interaction to emerge.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2012},
  keyword      = {Linguistics, Virtual Reality, Human-Computer Interaction, Deixis, Multimodal Communication},
  location     = {Orange County, CA, USA},
  pages        = {89--90},
  publisher    = {IEEE},
  title        = {{Towards a Linguistically Motivated Model for Selection in Virtual Reality}},
  year         = {2012},
}

@inbook{56,
  author       = {Lücking, Andy and Pfeiffer, Thies},
  booktitle    = {Handbook of Technical Communication},
  editor       = {Mehler, Alexander and Romary, Laurent},
  keyword      = {Multimodal Communication},
  pages        = {591--644},
  publisher    = {Mouton de Gruyter},
  title        = {{Framing Multimodal Technical Communication. With Focal Points in Speech-Gesture-Integration and Gaze Recognition}},
  doi          = {10.1515/9783110224948.591},
  volume       = {8},
  year         = {2012},
}

@inproceedings{57,
  abstract     = {The idea of using gaze as an interaction modality has been put forward by the famous work of Bolt in 1981. In virtual reality (VR), gaze has been used for several means since then: view-dependent optimization of rendering, intelligent information visualization, reference communication in distributed telecommunication settings and object selection. 
Our own research aims at improving gaze-based interaction methods in general. In this paper, gaze-based interaction is examined in a fast-paced selection task to identify current usability problems of gaze-based interaction and to develop best practices. To this end, an immersive Asteroids-like shooter called Eyesteroids was developed to support a study comparing manual and gaze-based interaction methods. Criteria for the evaluation were interaction performance and user immersion. The results indicate that while both modalities (hand and gaze) work well for the task, manual interaction is easier to use and often more accurate than the implemented gaze-based methods. The reasons are discussed and the best practices as well as options for further improvements of gaze-based interaction methods are presented.},
  author       = {Hülsmann, Felix and Dankert, Timo and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Workshop Virtuelle \& Erweiterte Realität 2011},
  editor       = {Bohn, Christian-A. and Mostafawy, Sina},
  keyword      = {Gaze-based Interaction},
  pages        = {1--12},
  publisher    = {Shaker Verlag},
  title        = {{Comparing gaze-based and manual interaction in a fast-paced gaming task in Virtual Reality}},
  year         = {2011},
}

@inproceedings{58,
  abstract     = {The idea of using gaze as an interaction modality has been put forward by the famous work of Bolt in 1981. In virtual reality (VR), gaze has been used for several means since then: view-dependent optimization of rendering, intelligent information visualization, reference communication in distributed telecommunication settings and object selection. 
Our own research aims at improving gaze-based interaction methods in general. In this paper, gaze-based interaction is examined in a fast-paced selection task to identify current usability problems of gaze-based interaction and to develop best practices. To this end, an immersive Asteroids-like shooter called Eyesteroids was developed to support a study comparing manual and gaze-based interaction methods. Criteria for the evaluation were interaction performance and user immersion. The results indicate that while both modalities (hand and gaze) work well for the task, manual interaction is easier to use and often more accurate than the implemented gaze-based methods. The reasons are discussed and the best practices as well as options for further improvements of gaze-based interaction methods are presented.},
  author       = {Renner, Patrick and Lüdike, Nico and Wittrowski, Jens and Pfeiffer, Thies},
  booktitle    = {Proceedings of the Workshop Virtuelle \& Erweiterte Realität 2011},
  editor       = {Bohn, Christian-A. and Mostafawy, Sina },
  keyword      = {Virtual Reality, Gaze-based Interaction},
  pages        = {13--24},
  publisher    = {Shaker Verlag},
  title        = {{Towards Continuous Gaze-Based Interaction in 3D Environments - Unobtrusive Calibration and Accuracy Monitoring}},
  year         = {2011},
}

@misc{59,
  author       = {Essig, Kai and Pfeiffer, Thies and Sand, Norbert and Künsemöller, Jörn and Ritter, Helge and Schack, Thomas},
  booktitle    = {Journal of Eye Movement Research},
  keyword      = {annotation, object recognition, computer vision, Gaze-based Interaction, eye-tracking},
  location     = {Marseille},
  number       = {3},
  pages        = {48},
  title        = {{JVideoGazer - Towards an Automatic Annotation of Gaze Videos from Natural Scenes}},
  volume       = {4},
  year         = {2011},
}

@article{60,
  abstract     = {Social networking platforms (SNPs) are meant to reflect the social relationships of their users. Users typically enter very personal information and should get useful feedback about their social network. They should indeed be empowered to exploit the information they have entered. In reality, however, most SNPs actually hide the structure of the user’s rich social network behind very restricted text-based user interfaces and large parts of the potential information which could be extracted from the entered data lies fallow. This article presents results from a user study showing that 3D visualizations of social graphs can be utilized more effectively – and moreover – are preferred by users compared to traditional text-based interfaces. Subsequently, the article addresses the problem of how to deploy interactive 3D graphical interfaces to large user communities. This is demonstrated on the social graph app-
lication FriendGraph3D for Facebook.},
  author       = {Mattar, Nikita and Pfeiffer, Thies},
  journal      = {International Journal of Computer Information Systems and Industrial Management Applications},
  keyword      = {information visualization, interactive graphs, social networks, web technology, 3d graphics},
  pages        = {427--434},
  title        = {{Interactive 3D graphs for web-based social networking platforms}},
  volume       = {3},
  year         = {2011},
}

@inproceedings{61,
  abstract     = {Since 2004 the virtual agent Max is living at the Heinz Nixdorf MuseumsForum – a computer science museum. He is welcoming and entertaining visitors ten hours a day, six days a week, for seven years. This article brings together the experiences made by the staff of the museum, the scientists who created and maintained the installation, the visitors and the agent himself. It provides insights about the installation’s hard- and software and presents highlights of the agent’s ontogenesis in terms of the features he has gained. A special focus is on the means Max uses to engage with visitors and the features which make him attractive.},
  author       = {Pfeiffer, Thies and Liguda, Christian and Wachsmuth, Ipke and Stein, Stefan},
  booktitle    = {Proceedings of the Re-Thinking Technology in Museums 2011 - Emerging Experiences},
  editor       = {Barbieri , Sara  and Scott, Katherine  and Ciolfi, Luigina},
  keyword      = {Embodied Conversational Agent, ECA, Chatterbot, Max, Museum, Artificial Intelligence, Virtual Agent},
  location     = {Limerick},
  pages        = {121--131},
  publisher    = {thinkk creative \& the University of Limerick},
  title        = {{Living with a Virtual Agent: Seven Years with an Embodied Conversational Agent at the Heinz Nixdorf MuseumsForum}},
  year         = {2011},
}

@inproceedings{62,
  abstract     = {Die Messung visueller Aufmerksamkeit mittels Eye-Tracking ist eine etablierte Methode in der Bewertung von Ergonomie und Usability. Ihr Gegenstandsbereich beschränkt sich jedoch primär auf 2D-Inhalte wie Webseiten, Produktfotos oder –videos. Bewegte Interaktion im dreidimensionalen Raum wird selten erfasst, weder am realen Objekt, noch am virtuellen Prototyp. Mit einer Aufmerksamkeitsmessung im Raum könnte der Gegenstandsbereich um diese Fälle deutlich erweitert werden. Der vorliegende Artikel arbeitet den aktuellen Stand der Forschung zur Messung visueller Aufmerksamkeit im Raum auf. Dabei werden insbesondere die zu bewältigenden Schwierigkeiten herausgearbeitet und Lösungsansätze aufgezeigt. Als Schwerpunkt werden drei Themen an eigenen Arbeiten diskutiert: Aufbau und Kalibrierung der Systeme, Bestimmung des betrachteten Volumens und Visualisierung der Aufmerksamkeit im Raum.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {10. Paderborner Workshop Augmented and Virtual Reality in der Produktentstehung},
  editor       = {Gausemeier, Jürgen and Grafe, Michael and Meyer auf der Heide, Friedhelm},
  keyword      = {Gaze-based Interaction},
  number       = {295},
  pages        = {39--51},
  publisher    = {Heinz Nixdorf Institut, Universität Paderborn},
  title        = {{Dreidimensionale Erfassung visueller Aufmerksamkeit für Usability-Bewertungen an virtuellen Prototypen}},
  year         = {2011},
}

@book{63,
  abstract     = {When humans communicate, we use deictic expressions to refer to objects in our surrounding and put them in the context of our actions. In face to face interaction, we can complement verbal expressions with gestures and, hence, we do not need to be too precise in our verbal protocols. Our interlocutors hear our speaking; see our gestures and they even read our eyes. They interpret our deictic expressions, try to identify the referents and – normally – they will understand. If only machines could do alike.

The driving vision behind the research in this thesis are multimodal conversational interfaces where humans are engaged in natural dialogues with computer systems. The embodied conversational agent Max developed in the A.I. group at Bielefeld University is an example of such an interface. Max is already able to produce multimodal deictic expressions using speech, gaze and gestures, but his capabilities to understand humans are not on par. If he was able to resolve multimodal deictic expressions, his understanding of humans would increase and interacting with him would become more natural. 

Following this vision, we as scientists are confronted with several challenges. First, accurate models for human pointing have to be found. Second, precise data on multimodal interactions has to be collected, integrated and analyzed in order to create these models. This data is multimodal (transcripts, voice and video recordings, annotations) and not directly accessible for analysis (voice and video recordings). Third, technologies have to be developed to support the integration and the analysis of the multimodal data. Fourth, the created models have to be implemented, evaluated and optimized until they allow a natural interaction with the conversational interface.

To this ends, this work aims to deepen our knowledge of human non-verbal deixis, specifically of manual and gaze pointing, and to apply this knowledge in conversational interfaces. At the core of the theoretical and empirical investigations of this thesis are models for the interpretation of pointing gestures to objects. These models address the following questions: When are we pointing? Where are we pointing to? Which objects are we pointing at? With respect to these questions, this thesis makes the following three contributions: First, gaze-based interaction technology for 3D environments: Gaze plays an important role in human communication, not only in deictic reference. Yet, technology for gaze interaction is still less developed than technology for manual interaction.

In this thesis, we have developed components for real-time tracking of eye movements and of the point of regard in 3D space and integrated them in a framework for Deictic Reference In Virtual Environments (DRIVE). DRIVE provides viable information about human communicative behavior in real-time. This data can be used to investigate and to design processes on higher cognitive levels, such as turn-taking, check-backs, shared attention and resolving deictic reference. 

Second, data-driven modeling: We answer the theoretical questions about timing, direction, accuracy and dereferential power of pointing by data-driven modeling.

As empirical basis for the simulations, we created a substantial corpus with highprecision data from an extensive study on multimodal pointing. Two further studies complemented this effort with substantial data on gaze pointing in 3D. Based on this data, we have developed several models of pointing and successfully created a model for the interpretation of manual pointing that achieves a human-like performance level.

Third, new methodologies for research on multimodal deixis in the fields of linguistics and computer science: The experimental-simulative approach to modeling – which we follow in this thesis – requires large collections of heterogeneous data to be recorded, integrated, analyzed and resimulated. To support the researcher in these tasks, we developed the Interactive Augmented Data Explorer (IADE). IADE is an innovative tool for research on multimodal interaction based on virtual reality technology. It allows researchers to literally immerse into multimodal data
and interactively explore them in real-time and in virtual space. With IADE we have also extended established approaches for scientific visualization of linguistic data to 3D, which previously existed only for 2D methods of analysis (e.g. video recordings or computer screen experiments). By this means, we extended Mc-Neill’s 2D depiction of the gesture space to gesture space volumes expanding in time and space. Similarly, we created attention volumes, a new way to visualize the distribution of attention in 3D environments.},
  author       = {Pfeiffer, Thies},
  keyword      = {Multimodal Communication, Gaze-based Interaction},
  pages        = {217},
  publisher    = {Shaker Verlag},
  title        = {{Understanding Multimodal Deixis with Gaze and Gesture in Conversational Interfaces}},
  year         = {2011},
}

@inproceedings{64,
  abstract     = {Referring to objects using multimodal deictic expressions is an important form of communication. This work addresses the question on how content is distributed between the modalities speech and gesture by comparing deictic pointing gestures to objects with and without speech. As a result, two main strategies used by participants to adapt their gestures to the condition were identified. This knowledge can be used, e.g., to improve the naturalness of pointing gestures employed by embodied conversational agents.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Gestures in Embodied Communication and Human-Computer Interaction, 9th International Gesture Workshop, GW 2011},
  editor       = {Efthimiou, Eleni and Kouroupetroglou, Georgios},
  keyword      = {Interaction between Speech and Gesture, Gesture, Speech, Pointing, Multimodal Fusion, Multimodal Communication},
  location     = {Athens, Greece},
  pages        = {109--112},
  publisher    = {National and Kapodistrian University of Athens},
  title        = {{Interaction between Speech and Gesture: Strategies for Pointing to Distant Objects}},
  year         = {2011},
}

@inproceedings{65,
  abstract     = {Solving word problems is an important part in school education in primary as well as in high school. Although, the equations that are given by a word problem could be solved by most computer algebra programs without problems, there are just few systems that are able to solve word problems. In this paper we present the ongoing work on a system, that is able to solve word problems from german primary school math books.},
  author       = {Liguda, Christian and Pfeiffer, Thies},
  booktitle    = {First International Workshop on Algorithmic Intelligence},
  editor       = {Messerschmidt, Hartmut},
  keyword      = {Artificial Intelligence, Natural Language Processing, Math Word Problems},
  location     = {Berlin},
  title        = {{A Question Answer System for Math Word Problems}},
  year         = {2011},
}

@inproceedings{66,
  abstract     = {Classic techniques for navigation and selection such as Image-Plane and World in Miniature have been around for more than 20 years. In the course of a seminar on interaction in virtual reality we reconsidered five methods for navigation and two for selection. These methods were significantly extended by the use of up-to-date hardware such as Fingertracking devices and the Nintendo Wii Balance Board and evaluated in a virtual supermarket scenario. Two user studies, one on experts and one on novices, revealed information on usability and efficiency. As an outcome, the combination of Ray-Casting and Walking in Place turned out to be the fastest.},
  author       = {Renner, Patrick and Dankert, Timo and Schneider, Dorothe and Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realitaet: 7. Workshop der GI-Fachgruppe VR/AR},
  keyword      = {human-machine-interaction
},
  location     = {Stuttgart},
  pages        = {71--82},
  publisher    = {Shaker Verlag},
  title        = {{Navigating and selecting in the virtual supermarket: review and update of classic interaction techniques}},
  year         = {2010},
}

@inproceedings{67,
  abstract     = {Humans perceive, reason and act within a 3D environment. In empirical methods, however, researchers often restrict themselves to 2D, either in using 2D content or relying on 2D recordings for analysis, such as videos or 2D eye movements. Regarding, e.g., multimodal deixis, we address the open question of the morphology of the referential space (Butterworth and Itakura, 2000), For modeling the referential space of gaze pointing, precise knowledge about the target of our participants’ visual attention is crucial. To this ends, we developed methods to assess the location of the point of regard, which are outlined here.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the KogWis 2010},
  editor       = {Haack, Johannes and Wiese, Heike and Abraham, Andreas and Chiarcos, Christian},
  keyword      = {Gaze-based Interaction},
  location     = {Potsdam, Germany},
  pages        = {220--221},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Tracking and Visualizing Visual Attention in Real 3D Space}},
  year         = {2010},
}

@inproceedings{68,
  abstract     = {From the perspective of individual users, social networking platforms (SNPs) are meant to reflect their social relationships. SNPs should provide feedback allowing users to exploit the information they have entered. In reality, however, most SNPs actually hide the rich social network constructed by the users in their databases behind simple user interfaces. These interfaces reduce the complexity of a user's social network to a text-based list in HTML. This article presents results from a user study showing that 3D visualizations of social graphs can be utilized more effectively – and moreover – are preferred by users compared to traditional text-based interfaces. Subsequently, the article addresses the problem of deployment of rich interfaces. A social graph application for Facebook is presented, demonstrating how WebGL and HTML5/X3D can be used to implement rich social applications based on upcoming web standards.},
  author       = {Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Proceedings of the IADIS International Conference Web Virtual Reality and Three-Dimensional Worlds 2010},
  keyword      = {social networks},
  pages        = {269--276},
  publisher    = {IADIS Press},
  title        = {{Relationships in social networks revealed: a facebook app for social graphs in 3D based on X3DOM and WebGL}},
  year         = {2010},
}

@phdthesis{69,
  abstract     = {When humans communicate, we use deictic expressions to refer to objects in our surrounding and put them in the context of our actions. In face to face interaction, we can complement verbal expressions with gestures and, hence, we do not need to be too precise in our verbal protocols. Our interlocutors hear our speaking; see our gestures and they even read our eyes. They interpret our deictic expressions, try to identify the referents and -- normally -- they will understand. If only machines could do alike.
The driving vision behind the research in this thesis are multimodal conversational interfaces where humans are engaged in natural dialogues with computer systems. The embodied conversational agent Max developed in the A.I. group at Bielefeld University is an example of such an interface. Max is already able to produce multimodal deictic expressions using speech, gaze and gestures, but his capabilities to understand humans are not on par. If he was able to resolve multimodal deictic expressions, his understanding of humans would increase and interacting with him would become more natural.
Following this vision, we as scientists are confronted with several challenges. First, accurate models for human pointing have to be found. Second, precise data on multimodal interactions has to be collected, integrated and analyzed in order to create these models. This data is multimodal (transcripts, voice and video recordings, annotations) and not directly accessible for analysis (voice and video recordings). Third, technologies have to be developed to support the integration and the analysis of the multimodal data. Fourth, the created models have to be implemented, evaluated and optimized until they allow a natural interaction with the conversational interface.
To this ends, this work aims to deepen our knowledge of human non-verbal deixis, specifically of manual and gaze pointing, and to apply this knowledge in conversational interfaces. At the core of the theoretical and empirical investigations of this thesis are models for the interpretation of pointing gestures to objects. These models address the following questions: When are we pointing? Where are we pointing to? Which objects are we pointing at? With respect to these questions, this thesis makes the following three contributions:
First, gaze-based interaction technology for 3D environments: Gaze plays an important role in human communication, not only in deictic reference. Yet, technology for gaze interaction is still less developed than technology for manual interaction. In this thesis, we have developed components for real-time tracking of eye movements and of the point of regard in 3D space and integrated them in a framework for DRIVE. DRIVE provides viable information about human communicative behavior in real-time. This data can be used to investigate and to design processes on higher cognitive levels, such as turn-taking, check- backs, shared attention and resolving deictic reference.
Second, data-driven modeling: We answer the theoretical questions about timing, direction, accuracy and dereferential power of pointing by data-driven modeling. As empirical basis for the simulations, we created a substantial corpus with high-precision data from an extensive study on multimodal pointing. Two further studies complemented this effort with substantial data on gaze pointing in 3D. Based on this data, we have developed several models of pointing and successfully created a model for the interpretation of manual pointing that achieves a human-like performance level.
Third, new methodologies for research on multimodal deixis in the fields of linguistics and computer science: The experimental-simulative approach to modeling -- which we follow in this thesis -- requires large collections of heterogeneous data to be recorded, integrated, analyzed and resimulated. To support the researcher in these tasks, we developed the Interactive Augmented Data Explorer. IADE is an innovative tool for research on multimodal interaction based on virtual reality technology. It allows researchers to literally immerse into multimodal data and interactively explore them in real-time and in virtual space. With IADE we have also extended established approaches for scientific visualization of linguistic data to 3D, which previously existed only for 2D methods of analysis (e.g. video recordings or computer screen experiments). By this means, we extended McNeill's 2D depiction of the gesture space to gesture space volumes expanding in time and space. Similarly, we created attention volumes, a new way to visualize the distribution of attention in 3D environments.},
  author       = {Pfeiffer, Thies},
  keyword      = {Reference, Gesture, Deixis, Human-Computer Interaction, Mensch-Maschine-Schnittstelle, Lokale Deixis, Blickbewegung, Gaze, Virtuelle Realität, Multimodales System, Referenz <Linguistik>, Gestik, Multimodal Communication, Gaze-based Interaction},
  pages        = {241},
  publisher    = {Universitätsbibliothek},
  title        = {{Understanding multimodal deixis with gaze and gesture in conversational interfaces}},
  year         = {2010},
}

@inproceedings{70,
  abstract     = {Object deixis is at the core of language and an ideal example of multimodality. Speech, gaze and manual gestures are used by interlocutors to refer to objects in their 3D environment. The interplay of verbal expressions and gestures during deixis is an active research topic in linguistics as well as in human-computer interaction. Previously, we conducted a study on manual pointing during dialogue games using state-of-the art tracking technologies to record gestures with high spatial precision (Kranstedt, Lücking, Pfeiffer, Rieser and Wachsmuth, 2006), To reveal strategies in manual pointing gestures, we present an analysis of this data with a new visualization technique.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Proceedings of the KogWis 2010},
  editor       = {Haack, Johannes and Wiese, Heike and Abraham, Andreas and Chiarcos, Christian},
  keyword      = {Multimodal Communication},
  location     = {Potsdam},
  pages        = {221--222},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Object Deixis: Interaction Between Verbal Expressions and Manual Pointing Gestures}},
  year         = {2010},
}

@inproceedings{71,
  abstract     = {Die Portale für soziale Netzwerke im Internet gehen mittlerweile deutlich über die Verwaltung einfacher Bekanntschaftsbeziehungen hinaus. Ihnen liegen immer reichhaltigere Datenmodelle zu Grunde. Darstellung und Exploration dieser Netzwerke sind eine grosse Herausforderung für die Entwickler, wenn beides nicht zu einer solchen für die Benutzer werden soll. Im Rahmen eines studentischen Projektes wurde die dritte Dimension für die Darstellung des komplexen sozialen Netzwerkes von Last.fm nutzbar gemacht. Durch die entwickelte Anwendung SoNAR wird das Netzwerk interaktiv und intuitiv sowohl am Desktop, als auch in der Immersion einer dreiseitigen CAVE explorierbar. Unterschiedliche Relationen des sozialen Graphen können parallel exploriert und damit Zusammenhänge zwischen Individuen intuitiv erfahren werden. Eine Suchfunktion erlaubt dabei die fexible Komposition verschiedener Startknoten für die Exploration.},
  author       = {Bluhm, Andreas and Eickmeyer, Jens and Feith, Tobias and Mattar, Nikita and Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität: 6. Workshop der GI-Fachgruppe VR/AR},
  keyword      = {social networks},
  pages        = {269--280},
  publisher    = {Shaker Verlag},
  title        = {{Exploration von sozialen Netzwerken im 3D Raum am Beispiel von SONAR für Last.fm}},
  year         = {2009},
}

@inproceedings{72,
  abstract     = {The "Where?" is quite important for Mixed Reality applications: Where is the user looking at? Where should augmentations be displayed? The location of the overt visual attention of the user can be used both to disambiguate referent objects and to inform an intelligent view management of the user interface. While the vertical and horizontal orientation of attention is quite commonly used, e.g. derived from the orientation of the head, only knowledge about the distance allows for an intrinsic measurement of the location of the attention. This contribution reviews our latest results on detecting the location of attention in 3D space using binocular eye tracking.},
  author       = {Pfeiffer, Thies and Mattar, Nikita},
  booktitle    = {Workshop-Proceedings der Tagung Mensch \& Computer 2009: Grenzenlos frei!?},
  keyword      = {Gaze-based Interaction},
  publisher    = {Logos Berlin},
  title        = {{Benefits of locating overt visual attention in space using binocular eye tracking for mixed reality applications}},
  year         = {2009},
}

@inproceedings{73,
  abstract     = {The Semantic Web is about to become a rich source of knowledge whose potential will be squandered if it is not accessible for everyone. Intuitive interfaces like conversational agents are needed to better disseminate this knowledge, either on request or even proactively in a context-aware manner. This paper presents work on extending an existing conversational agent, Max, with abilities to access the Semantic Web via natural language communication.},
  author       = {Breuing, Alexa and Pfeiffer, Thies and Kopp, Stefan},
  booktitle    = {Proceedings of the Poster and Demonstration Session at the 7th International Semantic Web Conference (ISWC 2008)},
  editor       = {Bizer, Christian and Joshi, Anupam},
  title        = {{Conversational Interface Agents for the Semantic Web - a Case Study}},
  year         = {2008},
}

@inproceedings{74,
  abstract     = {Interaction in conversational interfaces strongly relies on the sys- tem's capability to interpret the user's references to objects via de- ictic expressions. Deictic gestures, especially pointing gestures, provide a powerful way of referring to objects and places, e.g., when communicating with an Embodied Conversational Agent in a Virtual Reality Environment. We highlight results drawn from a study on pointing and draw conclusions for the implementation of pointing-based conversational interactions in partly immersive Vir- tual Reality.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the IEEE VR 2008},
  editor       = {Lin, Ming and Steed, Anthony and Cruz-Neira, Carolina},
  keyword      = {Multimodal Communication},
  pages        = {281--282},
  publisher    = {IEEE Press},
  title        = {{Conversational pointing gestures for virtual reality interaction: Implications from an empirical study}},
  doi          = {10.1109/vr.2008.4480801},
  year         = {2008},
}

@article{75,
  abstract     = {Tracking user's visual attention is a fundamental aspect in novel human-computer interaction paradigms found in Virtual Reality. For example, multimodal interfaces or dialogue-based communications with virtual and real agents greatly benefit from the analysis of the user's visual attention as a vital source for deictic references or turn-taking signals. Current approaches to determine visual attention rely primarily on monocular eye trackers. Hence they are restricted to the interpretation of two-dimensional fixations relative to a defined area of projection. The study presented in this article compares precision, accuracy and application performance of two binocular eye tracking devices. Two algorithms are compared which derive depth information as required for visual attention-based 3D interfaces. This information is further applied to an improved VR selection task in which a binocular eye tracker and an adaptive neural network algorithm is used during the disambiguation of partly occluded objects.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich and Wachsmuth, Ipke},
  journal      = {JVRB - Journal of Virtual Reality and Broadcasting},
  keyword      = {human-computer interaction, object selection, virtual reality, gaze-based interaction, eye tracking},
  number       = {16},
  pages        = {1660},
  title        = {{Evaluation of binocular eye trackers and algorithms for 3D gaze interaction in virtual reality environments}},
  volume       = {5},
  year         = {2008},
}

@inproceedings{76,
  abstract     = {Of all senses, it is visual perception that is predominantly deluded in Virtual Realities. Yet, the eyes of the observer, despite the fact that they are the fastest perceivable moving body part, have gotten relatively little attention as an interaction modality. A solid integration of gaze, however, provides great opportunities for implicit and explicit human-computer interaction. We present our work on integrating a lightweight head-mounted eye tracking system in a CAVE-like Virtual Reality Set-Up and provide promising data from a user study on the achieved accuracy and latency.},
  author       = {Pfeiffer, Thies},
  booktitle    = {Virtuelle und Erweiterte Realität - Fünfter Workshop der GI-Fachgruppe VR/AR},
  editor       = {Schumann, Marco and Kuhlen, Torsten},
  keyword      = {Gaze-based Interaction},
  pages        = {81--92},
  publisher    = {Shaker Verlag GmbH},
  title        = {{Towards Gaze Interaction in Immersive Virtual Reality: Evaluation of a Monocular Eye Tracking Set-Up}},
  year         = {2008},
}

@inproceedings{77,
  abstract     = {Emotions and interpersonal distances are identified as key aspects in social interaction. A novel Affective Computer-Mediated Communication (ACMC) framework has been developed making the interplay of both aspects explicit to facilitate social presence. In this ACMC framework, the displays can be arranged in virtual space manually or automatically. We expect that, according to empirical findings, the social relation as well as momentarily affective appraisals will influence this arrangement. The proposed concept extends from desktop devices to fully immersive Virtual Reality interfaces.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Proceedings of the 11th International Workshop on Presence},
  editor       = {Spagnolli, Anna and Gamberini, Luciano},
  keyword      = {Mediated Communication},
  pages        = {275--279},
  publisher    = {CLEUP Cooperativa Libraria Universitaria Padova},
  title        = {{Social Presence: The Role of Interpersonal Distances in Affective Computer-Mediated Communication}},
  year         = {2008},
}

@inproceedings{78,
  abstract     = {People engaged in successful dialog have to share knowledge, e.g., when naming objects, for coordinating their actions. According to Clark (1996), this shared knowledge, the common ground, is explicitly established, particularly by negotiations. Pickering and Garrod (2004) propose with their alignment approach a more automatic and resource-sensitive mechanism based on priming. Within the collaborative research center (CRC) "Alignment in Communication" a series of experimental investigations of natural Face-to-face dialogs should bring about vital evidence to arbitrate between the two positions. This series should ideally be based on a common setting. In this article we review experimental settings in this research line and refine a set of requirements. We then present a flexible design called the Jigsaw Map Game and demonstrate its applicability by reporting on a first experiment on object naming.},
  author       = {Weiß, Petra and Pfeiffer, Thies and Schaffranietz, Gesche and Rickheit, Gert},
  booktitle    = {Cognitive Science 2007: Proceedings of the 8th Annual Conference of the Cognitive Science Society of Germany},
  editor       = {Zimmer, Hubert D. and Frings, C. and Mecklinger, Axel and Opitz, B. and Pospeschill, M. and Wentura, D.},
  keyword      = {Multimodal Communication},
  location     = {Saarbrücken, Germany},
  title        = {{Coordination in dialog: Alignment of object naming in the Jigsaw Map Game}},
  year         = {2008},
}

@misc{79,
  author       = {Meißner, Martin and Essig, Kai and Pfeiffer, Thies and Decker, Reinhold and Ritter, Helge},
  booktitle    = {Perception},
  keyword      = {In a novel approach we investigated choice processes using eye tracking to improve research instruments in marketing research. Choice-based conjoint analysis (CBC) is the most widely-used tool for investigating consumer preferences on the basis of choice tasks. While CBC is highly appreciated for its realism (Haaijer and Wedel, 2007), marketing researchers have highlighted that respondents are easily exposed to the problem of information overload (Green et al, 2001). The question how much information is being processed during choice processes and how preference measurement is affected remains an open research issue. We investigated choice processes using eye tracking in a CBC on-line consumer survey. We showed (i) that the extent to which information is processed is decreasing in later choice tasks, (ii) in how far information overload changes the pattern of eye movements, and (iii) how the difficulty of a choice task influences information processing.},
  pages        = {97--97},
  publisher    = {PION LTD},
  title        = {{Eye-tracking decision behaviour in choice-based conjoint analysis}},
  volume       = {37},
  year         = {2008},
}

@inproceedings{80,
  abstract     = {Für die Mensch-Maschine-Interaktion ist die Erfassung der Aufmerksamkeit des Benutzers von großem Interesse. Für Anwendungen in der Virtuellen Realität (VR) gilt dies insbesondere, nicht zuletzt dann, wenn Virtuelle Agenten als Benutzerschnittstelle eingesetzt werden. Aktuelle Ansätze zur Bestimmung der visuellen Aufmerksamkeit verwenden meist monokulare Eyetracker und interpretieren daher auch nur zweidimensionale bedeutungstragende Blickfixationen relativ zu einer Projektionsebene. Für typische Stereoskopie-basierte VR Anwendungen ist aber eine zusätzliche Berücksichtigung der Fixationstiefe notwendig, um so den Tiefenparameter für die Interaktion nutzbar zu machen, etwa für eine höhere Genauigkeit bei der Objektauswahl (Picking). Das in diesem Beitrag vorgestellte Experiment zeigt, dass bereits mit einem einfacheren binokularen Gerät leichter zwischen sich teilweise verdeckenden Objekten unterschieden werden kann. Trotz des positiven Ergebnisses kann jedoch noch keine uneingeschränkte Verbesserung der Selektionsleistung gezeigt werden. Der Beitrag schließt mit einer Diskussion nächster Schritte mit dem Ziel, die vorgestellte Technik weiter zu verbessern.},
  author       = {Pfeiffer, Thies and Donner, Matthias and Latoschik, Marc Erich and Wachsmuth, Ipke},
  booktitle    = {Virtuelle und Erweiterte Realität. 4. Workshop der GI-Fachgruppe VR/AR},
  editor       = {Latoschik, Marc Erich and Fröhlich, Bernd},
  keyword      = {Mensch-Maschine-Interaktion, Eyetracking, Virtuelle Realität},
  pages        = {113--124},
  publisher    = {Shaker},
  title        = {{Blickfixationstiefe in stereoskopischen VR-Umgebungen: Eine vergleichende Studie}},
  year         = {2007},
}

@article{81,
  abstract     = {Humans perceive and act within a visual world. This has become important even in
disciplines which are prima facie not concerned with vision and eye tracking is now used
in a broad range of domains. However, the world we are in is not two-dimensional, as
many experiments may convince us to believe. It is convenient to use stimuli in 2D or
21/2D and in most cases absolutely appropriate; but it is often technically motivated and
not scientifically.
To overcome these technical limitations we contribute results of an evaluation of different
approaches to calculate the depth of a fixation based on the divergence of the eyes
by testing them on different devices and within real and virtual scenarios.},
  author       = {Pfeiffer, Thies and Donner, Matthias and Latoschik, Marc Erich and Wachsmuth, Ipke},
  journal      = {Journal of Eye Movement Research},
  keyword      = {Gaze-based Interaction},
  pages        = {13},
  title        = {{3D fixations in real and virtual scenarios}},
  volume       = {Special issue: Abstracts of the ECEM 2007},
  year         = {2007},
}

@inproceedings{82,
  abstract     = {Im Rahmen der Entwicklung einer multimodalen Schnittstelle für die Mensch-Maschine Kommunikation konzentriert sich diese Arbeit auf die Interpretation von Referenzen auf sichtbare Objekte. Im Vordergrund stehen dabei Fragen zur Genauigkeit von Zeigegesten und deren Interaktion mit sprachlichen Ausdrücken. Die Arbeit spannt dabei methodisch einen Bogen von Empirie über Simulation und Visualisierung zur Modellbildung und Evaluation. In Studien zur deiktischen Objektreferenz wurden neben sprachlichen Äußerungen unter dem Einsatz moderner Motion Capturing Technik umfangreiche Daten zum deiktischen Zeigen erhoben. Diese heterogenen Daten, bestehend aus Tracking Daten, sowie Video und Audio Aufzeichnungen, wurden annotiert und mit eigens entwickelten interaktiven Werkzeugen unter Einsatz von Techniken der Virtuellen Realität integriert und aufbereitet. Die statistische Auswertung der Daten erfolgte im Anschluß mittels der freien Statistik-Software R. Die datengetriebene Modellbildung bildet die Grundlage für die Weiterentwicklung eines unscharfen, fuzzy-basierten, Constraint Satisfaction Ansatzes zur Interpretation von Objektreferenzen. Wesentliches Ziel ist dabei eine inkrementelle, echtzeitfähige Verarbeitung, die den Einsatz in direkter Mensch-Maschine Interaktion erlaubt. Die Ergebnisse der Studie haben über die Fragestellung hinaus Einfluss auf ein Modell zur Produktion von deiktischen Ausdrücken und direkte Konsequenzen für einschlägige Theorien zur deiktischen Referenz.},
  author       = {Pfeiffer, Thies and Wachsmuth, Ipke},
  booktitle    = {Kognitionsforschung 2007 - Beiträge zur 8. Jahrestagung der Gesellschaft für Kognitionswissenschaft},
  editor       = {Frings, Christian and Mecklinger, Axel and Opitz, Bertram and Pospeschill, Markus and Wentura, Dirk and Zimmer, Hubert D.},
  keyword      = {multimodal communication},
  pages        = {109--110},
  publisher    = {Shaker Verlag},
  title        = {{Interpretation von Objektreferenzen in multimodalen Äußerungen}},
  year         = {2007},
}

@inproceedings{83,
  abstract     = {People engaged in successful dialog have to share knowledge, e.g., when naming objects, for coordinating their actions. According to Clark (1996), this shared knowledge, the common ground, is explicitly established, particularly by negotiations. Pickering and Garrod (2004) propose with their alignment approach a more automatic and resource-sensitive mechanism based on priming. Within the collaborative research center (CRC) “Alignment in Communication” a series of experimental investigations of natural face-to-face dialogs should bring about vital evidence to arbitrate between the two positions. This series should ideally be based on a common setting. In this article we review experimental settings in this research line and refine a set of requirements. We then present a flexible design called the Jigsaw Map Game and demonstrate its applicability by reporting on a first experiment on object naming.},
  author       = {Schaffranietz, Gesche and Weiß, Petra and Pfeiffer, Thies and Rickheit, Gert},
  booktitle    = {Kognitionsforschung 2007 - Beiträge zur 8. Jahrestagung der Gesellschaft für Kognitionswissenschaft},
  editor       = {Frings, Christian and Mecklinger, Axel and Opitz, Betram and Pospeschill, Markus and Wentura, Dirk and Zimmer, Hubert D.},
  keyword      = {Multimodal Communication},
  pages        = {41--42},
  publisher    = {Shaker Verlag},
  title        = {{Ein Experiment zur Koordination von Objektbezeichnungen im Dialog}},
  year         = {2007},
}

@inproceedings{84,
  abstract     = {The mediation of social presence is one of the most interesting challenges of modern communication technology. The proposed metaphor of Interactive Social Displays describes new ways of interactions with multi-/crossmodal interfaces prepared for a psychologically augmented communication. A first prototype demonstrates the application of this metaphor in a teleconferencing scenario.},
  author       = {Pfeiffer, Thies and Latoschik, Marc Erich},
  booktitle    = {IPT-EGVE 2007, Virtual Environments 2007, Short Papers and Posters},
  editor       = {Fröhlich, Bernd and Blach, Roland and Liere van, Robert},
  keyword      = {Mediated Communication},
  pages        = {41--42},
  publisher    = {Eurographics Association},
  title        = {{Interactive Social Displays}},
  year         = {2007},
}

@misc{85,
  abstract     = {The mediation of social presence is one of the most interesting challenges of modern communication technology. The proposed metaphor of Interactive Social Displays describes new ways of interactions with multi-/crossmodal interfaces prepared for a psychologically augmented communication. A first prototype demonstrates the application of this metaphor in a teleconferencing scenario.},
  author       = {Pfeiffer, Thies and Latoschik, Marc E.},
  keyword      = {Mediated Communication},
  title        = {{Interactive Social Displays}},
  year         = {2007},
}

@inbook{86,
  abstract     = {This chapter presents an original approach towards a detailed understanding of the usage of pointing gestures accompanying referring expressions. This effort is undertaken in the context of human-machine interaction integrating empirical studies, theory of grammar and logics, and simulation techniques. In particular, we take steps to classify the role of pointing in deictic expressions and to model the focussed area of pointing gestures, the so-called pointing cone. This pointing cone serves as a central concept in a formal account of multi-modal integration at the linguistic speech-gesture interface as well as in a computational model of processing multi-modal deictic expressions.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Wachsmuth, Ipke},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {155--208},
  publisher    = {Mouton de Gruyter},
  title        = {{Deictic object reference in task-oriented dialogue}},
  doi          = {10.1515/9783110197747.155},
  year         = {2006},
}

@inbook{87,
  abstract     = {This contribution presents investigations of the usage of computer gene¬rated 3D stimuli for psycholinguistic experiments. In the first part, we introduce VDesigner. VDesigner is a visual programming environment that operates in two different modes, a design mode to implement the materials and the structure of an experiment, and a runtime mode to actually run the experiment. We have extended VDesigner to support interactive experimentation in 3D. In the second part, we de-scribe a practical application of the programming environment. We have replicated a previous 2½D study of the production of spatial terms in a 3D setting, with the objective of investigating the effect of the presentation modes (2½D vs. 3D) on the choice of the referential system. In each trial, on being presented with a scene, the participants had to verbally specify the position of a target object in relation to a reference object. We recorded the answers of the participants as well as their reac-tion times. The results suggest that stereoscopic 3D presentations are a promising technology to elicit a more natural behavior of participants in computer-based experiments.},
  author       = {Flitter, Helmut and Pfeiffer, Thies and Rickheit, Gert},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {127--153},
  publisher    = {Mouton de Gruyter},
  title        = {{Psycholinguistic experiments on spatial relations using stereoscopic presentation}},
  year         = {2006},
}

@inproceedings{88,
  abstract     = {Für die empirische Erforschung situierter natürlicher menschlicher Kommunikation sind wir auf die Akquise und Auswertung umfangreicher Daten angewiesen. Die Modalitäten, über die sich Menschen ausdrücken können, sind sehr unterschiedlich. Entsprechend heterogen sind die Repräsentationen, mit denen die erhobenen Daten für die Auswertung verfügbar gemacht werden können. Für eine Untersuchung des Zeigeverhaltens bei der Referenzierung von Objekten haben wir mit IADE ein Framework für die Aufzeichnung, Analyse und Simulation von Sprach-Gestik Daten entwickelt. Durch den Einsatz von Techniken aus der interaktiven VR erlaubt IADE die synchronisierte Aufnahme von Bewegungs-, Video- und Audiodaten und unterstützt einen iterativen Auswertungsprozess der gewonnenen Daten durch komfortable integrierte Revisualisierungen und Simulationen. Damit stellt IADE einen entscheidenden Fortschritt für unsere linguistische Experimentalmethodik dar.},
  author       = {Pfeiffer, Thies and Kranstedt, Alfred and Lücking, Andy},
  booktitle    = {Dritter Workshop Virtuelle und Erweiterte Realität der GI-Fachgruppe VR/AR},
  editor       = {Müller, Stefan and Zachmann, Gabriel},
  keyword      = {Multimodal Communication},
  pages        = {61--72},
  publisher    = {Shaker},
  title        = {{Sprach-Gestik Experimente mit IADE, dem Interactive Augmented Data Explorer}},
  year         = {2006},
}

@inproceedings{89,
  abstract     = {We present a collaborative approach towards a detailed understanding of the usage of pointing gestures accompanying referring expressions. This effort is undertaken in the context of human-machine interaction integrating empirical studies, theory of grammar and logics, and simulation techniques. In particular, we attempt to measure the precision of the focussed area of a pointing gesture, the so-called pointing cone. ne pointing cone serves as a central concept in a formal account of multi-modal integration at the linguistic speech-gesture interface as well as in a computational model of processing multi-modal deictic expressions.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Wachsmuth, Ipke},
  booktitle    = {Gesture in Human-Computer Interaction and Simulation},
  editor       = {Gibet , Sylvie and Courty , Nicolas and Kamp , Jean-François},
  keyword      = {Multimodal Communication},
  pages        = {300--311},
  publisher    = {Springer},
  title        = {{Deixis: How to determine demonstrated objects using a pointing cone}},
  doi          = {10.1007/11678816_34},
  year         = {2006},
}

@inbook{90,
  abstract     = {Instructions play an important role in everyday communication, e.g. in task-oriented dialogs. Based on a (psycho-)linguistic theoretical background, which classifies instructions as requests, we conducted experiments using a cross-modal experimental design in combination with a reaction time paradigm in order to get insights in human instruction processing. We concentrated on the interpretation of basic single sentence instructions. Here, we especially examined the effects of the specificity of verbs, object names, and prepositions in interaction with factors of the visual object context regarding an adequate reference resolution. We were able to show that linguistic semantic and syntactic factors as well as visual context information context influence the interpretation of instructions. Especially the context information proves to be very important. Above and beyond the relevance for basic research, these results are also important for the design of human-computer interfaces capable of understanding natural language. Thus, following the experimental-simulative approach, we also pursued the processing of instructions from the perspective of computer science. Here, a natural language processing interface created for a virtual reality environment served as basis for the simulation of the empirical findings. The comparison of human vs. virtual system performance using a local performance measure for instruction understanding based on fuzzy constraint satisfaction led to further insights concerning the complexity of instruction processing in humans and artificial systems. Using selected examples, we were able to show that the visual context has a comparable influence on the performance of both systems, whereas this approach is limited when it comes to explaining some effects due to variations of the linguistic structure. In order to get deeper insights into the timing and interaction of the sub-processes relevant for instruction understanding and to model these effects in the computer simulation, more specific data on human performance are necessary, e.g. by using eye-tracking techniques. In the long run, such an approach will result in the development of a more natural and cognitively adequate human-computer interface.},
  author       = {Weiß, Petra and Pfeiffer, Thies and Eikmeyer, Hans-Jürgen and Rickheit, Gert},
  booktitle    = {Situated Communication},
  editor       = {Rickheit, Gert and Wachsmuth, Ipke},
  keyword      = {Multimodal Communication},
  pages        = {31--76},
  publisher    = {Mouton de Gruyter},
  title        = {{Processing Instructions}},
  year         = {2006},
}

@inproceedings{91,
  abstract     = {We describe an experiment to gather original data on geometrical aspects of pointing. In particular, we are focusing upon the concept of the pointing cone, a geometrical model of a pointing’s extension. In our setting we employed methodological and technical procedures of a new type to integrate data from annotations as well as from tracker recordings. We combined exact information on position and orientation with rater’s classifications. Our first results seem to challenge classical linguistic and philosophical theories of demonstration in that they advise to separate pointings from reference.},
  author       = {Kranstedt, Alfred and Lücking, Andy and Pfeiffer, Thies and Rieser, Hannes and Staudacher, Marc},
  booktitle    = {Proceedings of the brandial 2006 - The 10th Workshop on the Semantics and Pragmatics of Dialogue},
  editor       = {Schlangen, David and Fernández, Raquel},
  keyword      = {Multimodal Communication},
  pages        = {82--89},
  publisher    = {Universitätsverlag Potsdam},
  title        = {{Measuring and Reconstructing Pointing in Visual Contexts}},
  year         = {2006},
}

@inproceedings{92,
  abstract     = {Recently videoconferencing has been extended from human face-to-face communication to human machine interaction with Virtual Environments (VE)[6]. Relying on established videoconferencing (VC) protocol standards this thin client solution does not require specialised 3D soft- or hardware and scales well to multimedia enabled mobile devices. This would bring a whole range of new applications to the mobile platform. To facilitate our research in mobile interaction the Open Source project P@CE has been started to bring a fullfeatured VC client to the Pocket PC platform.},
  author       = {Weber, Matthias and Pfeiffer, Thies and Jung, Bernhard},
  booktitle    = {MOBILE HCI 05 Proceedings of the 7th International Conference on Human Computer Interaction with Mobile Devices and Services},
  editor       = {Tscheligi, Manfred and Bernhaupt, Regina and Mihalic, Kristijan},
  keyword      = {Mediated Communication},
  pages        = {351--352},
  publisher    = {ACM},
  title        = {{Pr@senZ - P@CE: Mobile Interaction with Virtual Reality}},
  year         = {2005},
}

@inproceedings{93,
  abstract     = {This paper presents an alternative to existing methods for remotely accessing Virtual Reality (VR) systems. Common solutions are based on specialised software and/or hardware capable of rendering 3D content, which not only restricts accessibility to specific platforms but also increases the barrier for non expert users. Our approach addresses new audiences by making existing Virtual Environments (VEs) ubiquitously accessible. Its appeal is that a large variety of clients, like desktop PCs and handhelds, are ready to connect to VEs out of the box. We achieve this combining established videoconferencing protocol standards with a server based interaction handling. Currently interaction is based on natural speech, typed textual input and visual feedback, but extensions to support natural gestures are possible and planned. This paper presents the conceptual framework enabling videoconferencing with collaborative VEs as well as an example application for a virtual prototyping system.},
  author       = {Pfeiffer, Thies and Weber, Matthias and Jung, Bernhard},
  booktitle    = {Theory and Practice of Computer Graphics 2005},
  editor       = {Lever, Louise and McDerby, Marc},
  keyword      = {Mediated Communication},
  pages        = {209--216},
  publisher    = {Eurographics Association},
  title        = {{Ubiquitous Virtual Reality: Accessing Shared Virtual Environments through Videoconferencing Technology}},
  year         = {2005},
}

@inproceedings{94,
  abstract     = {This paper describes the underlying concepts and the technical implementation of a system for resolving multimodal references in Virtual Reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a socalled reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.},
  author       = {Pfeiffer, Thies and Latoschik, M. E.},
  booktitle    = {Proceedings of the IEEE Virtual Reality 2004},
  editor       = {Ikei, Yasushi and Göbel, Martin and Chen, Jim},
  keyword      = {inform::Constraint Satisfaction inform::Multimodality inform::Virtual Reality ling::Natural Language Processing ling::Reference Resolution, Multimodal Communication},
  pages        = {35--42},
  title        = {{Resolving Object References in Multimodal Dialogues for Immersive Virtual Environments}},
  year         = {2004},
}

@inproceedings{95,
  abstract     = {This poster presents cognitive-motivated aspects of a technical system for the resolution of references to objects within an assembly-task domain. The research is integrated in the Collaborative Research Center SFB 360 which is concerned with situated artificial communicators. One application scenario consists of a task-oriented discourse between an instructor and a constructor who collaboratively build aggregates from a wooden toy kit (Baufix), or from generic CAD parts. In our current setting this scenario is embedded in a virtual reality (VR) installation, where the human user, taking the role of the instructor, guides the artificial constructor (embodied by the ECA Max) through the assembly process by means of multimodal task descriptions (see Figure 1). The system handles instructions like: Plug the left red screw from above in the middle hole of the wing and turn it this way. accompanied by coverbal deictic and mimetic gestures (see Latoschik, 2001).},
  author       = {Pfeiffer, Thies and Voss, Ian and Latoschik, Marc Erich},
  booktitle    = {Proceedings of the EuroCogSci03},
  editor       = {Schmalhofer, F. and Young, R.},
  keyword      = {inform::Multimodality inform::Virtual Reality ling::Reference Resolution ling::Natural Language Processing, Artificial Intelligence, Multimodal Communication},
  pages        = {426},
  publisher    = {Lawrence Erlbaum Associates Inc},
  title        = {{Resolution of Multimodal Object References using Conceptual Short Term Memory}},
  year         = {2003},
}

@misc{96,
  author       = {Pfeiffer, Thies},
  keyword      = {inform::Constraint Satisfaction inform::Multimodality inform::Virtual Reality ling::Natural Language Processing, Multimodal Communication},
  publisher    = {Faculty of Technology, University of Bielefeld},
  title        = {{Eine Referenzauflösung für die dynamische Anwendung in Konstruktionssituationen in der Virtuellen Realität}},
  year         = {2003},
}

@misc{97,
  abstract     = {When merging different knowledge bases one has to cope with the problem of classifying and linking concepts as well as the possibly heterogeneous representations the knowledge is expressed in. We are presenting an implementation that follows the Model Driven Architecture (MDA) [Miller and Mukerji, 2003] approach defined by the Object Management Group (OMG). Metamodels defined in the Unified Modeling Language (UML) are used to implement different knowledge representation formalisms. Knowledge is expressed as a Model instantiating the Metamodel. Integrating Metamodels are defined for merging knowledge distributed over different knowledge bases.},
  author       = {Pfeiffer, Thies and Voss, Ian},
  keyword      = {Artificial Intelligence},
  title        = {{Integrating Knowledge Bases Using UML Metamodels}},
  year         = {2003},
}

@inproceedings{98,
  abstract     = {This contribution describes a WWW-based multi-user system for concurrent virtual prototyping. A 3D scene of CAD parts is presented to the users in the web browser. By instructing the system using simple natural language commands, complex aggregates can be assembled from the basic parts. The current state of the assembly is instantly published to all system users who can discuss design choices in a chat area. The implementation builds on an existing system for virtual assembly made available as a web service. The client side components are fully implemented as Java applets and require no plugin for visualization of 3D content. Http tunneled messaging between web clients and server ensures system accessibility from any modern web browser even behind firewalls. The system is first to demonstrate natural language based virtual prototyping on the web.},
  author       = {Jung, Bernhard and Pfeiffer, Thies and Zakotnik, Jure},
  booktitle    = {Proceedings Structured Design of Virtual Environments and 3D-Components},
  editor       = {Geiger et al., C.},
  keyword      = {inform::Web inform::Internet inform::Collaborative Environments},
  pages        = {101--110},
  publisher    = {Shaker},
  title        = {{Natural Language Based Virtual Prototyping on the Web}},
  year         = {2002},
}




@INPROCEEDINGS{8797722,
  author={Onuki, Yoshikazu and Kumazawa, Itsuo},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Reorient the Gazed Scene Towards the Center: Novel Virtual Turning Using Head and Gaze Motions and Blink}, 
  year={2019},
  volume={},
  number={},
  pages={1864-1871},
  keywords={Turning;Visualization;Games;Strain;User experience;Face;Human-centered computing—Interaction paradigms—Virtual reality;Human-centered computing—Interaction design—Interaction design process and methods—Interface design prototyping},
  doi={10.1109/VR.2019.8797722}}


@article{10.1007/s10055-020-00425-x,
author = {Farmani, Yasin and Teather, Robert J.},
title = {Evaluating discrete viewpoint control to reduce cybersickness in virtual reality},
year = {2020},
issue_date = {Dec 2020},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {4},
issn = {1359-4338},
url = {https://doi.org/10.1007/s10055-020-00425-x},
doi = {10.1007/s10055-020-00425-x},
abstract = {Cybersickness in virtual reality (VR) is an ongoing problem, despite recent advances in head-mounted displays (HMDs). Discrete viewpoint control techniques have been recently used by some VR developers to combat cybersickness. Discrete viewpoint techniques rely on reducing optic flow via inconsistent displacement, to reduce cybersickness when using stationary HMD-based VR systems. However, reports of their effectiveness are mostly anecdotal. We experimentally evaluate two discrete movement techniques; we refer to as rotation snapping and translation snapping. We conducted two experiments measuring participant cybersickness levels via the widely used simulator sickness questionnaire (SSQ), as well as user-reported levels of nausea, presence, and objective error rates. Our results indicate that both rotation snapping and translation snapping significantly reduced SSQ by 40\% for rotational viewpoint movement, and 50\% for translational viewpoint movement. They also reduced participant nausea levels, especially with longer VR exposure. Presence levels, error rates, and performance were not significantly affected by either technique.},
journal = {Virtual Real.},
month = {dec},
pages = {645–664},
numpages = {20},
keywords = {Visually induced motion sickness, Cybersickness, Vection, Virtual reality}
}



@inproceedings{10.1145/3441852.3471230,
author = {L. Franz, Rachel and Junuzovic, Sasa and Mott, Martez},
title = {Nearmi: A Framework for Designing Point of Interest Techniques for VR Users with Limited Mobility},
year = {2021},
isbn = {9781450383066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3441852.3471230},
doi = {10.1145/3441852.3471230},
abstract = {We propose Nearmi, a framework that enables designers to create customizable and accessible point-of-interest (POI) techniques in virtual reality (VR) for people with limited mobility. Designers can use Nearmi by creating and combining instances of its four components—representation, display, selection, and transition. These components enable users to gain awareness of POIs in virtual environments, and automatically re-orient the virtual camera toward a selected POI. We conducted a video elicitation study where 17 participants with limited mobility provided feedback on different Nearmi implementations. Although participants generally weighed the same design considerations when discussing their preferences, their choices reflected tradeoffs in accessibility, realism, spatial awareness, comfort, and familiarity with the interaction. Our findings highlight the need for accessible and customizable VR interaction techniques, as well as design considerations for building and evaluating these techniques.},
booktitle = {Proceedings of the 23rd International ACM SIGACCESS Conference on Computers and Accessibility},
articleno = {5},
numpages = {14},
keywords = {Accessibility, Design framework, Limited mobility, Out-of-view, Point of interest, Virtual reality},
location = {Virtual Event, USA},
series = {ASSETS '21}
}


@inproceedings{10.1145/2159365.2159386,
author = {Folmer, Eelke and Liu, Fangzhou and Ellis, Barrie},
title = {Navigating a 3D avatar using a single switch},
year = {2011},
isbn = {9781450308045},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2159365.2159386},
doi = {10.1145/2159365.2159386},
abstract = {Many users with severe motor impairments, such as quadriplegics, interact with computers using an indirect selection technique called switch access scanning. Switch scanning allows for iteratively selecting an input from a set of input options using a single switch input and which replaces the use of a keyboard or a mouse, which they may be unable to use. Navigating an avatar in a 3D virtual world using existing switch access scanning systems is slow and erroneous because these interfaces are non-linear and requires players to provide continuous (holding a key) and mixed inputs (holding two or more keys). Through the analysis of navigation behavior of eight able-bodied users, a new scanning system called hold-and-release was developed. Using simulation hold-and-release scanning was found to be significantly more efficient than existing scanning systems. Multistep selection was found to be most efficient for mixing inputs, but expanding the selection set has no approximation errors.},
booktitle = {Proceedings of the 6th International Conference on Foundations of Digital Games},
pages = {154–160},
numpages = {7},
keywords = {motor impairment, scanning, switch access, virtual worlds},
location = {Bordeaux, France},
series = {FDG '11}
}


@incollection{cook_chapter_2015,
	address = {St. Louis (MO)},
	title = {Chapter 6 - {Making} the {Connection}: {User} {Inputs} for {Assistive} {Technologies}},
	isbn = {978-0-323-09631-7},
	shorttitle = {Chapter 6 - {Making} the {Connection}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323096317000065},
	urldate = {2024-07-22},
	booktitle = {Assistive {Technologies} ({Fourth} {Edition})},
	publisher = {Mosby},
	author = {Cook, Albert M. and Polgar, Janice M.},
	editor = {Cook, Albert M. and Polgar, Janice M.},
	month = jan,
	year = {2015},
	doi = {10.1016/B978-0-323-09631-7.00006-5},
	pages = {117--138},
	file = {Cook und Polgar - 2015 - Chapter 6 - Making the Connection User Inputs for.pdf:C\:\\Users\\Finja\\Zotero\\storage\\54DTYK3D\\Cook und Polgar - 2015 - Chapter 6 - Making the Connection User Inputs for.pdf:application/pdf},
}

@book{jerald_vr_2015,
	title = {The {VR} {Book}: {Human}-{Centered} {Design} for {Virtual} {Reality}},
	isbn = {978-1-970001-12-9},
	shorttitle = {The {VR} {Book}},
	abstract = {Virtual reality (VR) can provide our minds with direct access to digital media in a way that seemingly has no limits. However, creating compelling VR experiences is an incredibly complex challenge. When VR is done well, the results are brilliant and pleasurable experiences that go beyond what we can do in the real world. When VR is done badly, not only is the system frustrating to use, but it can result in sickness. There are many causes of bad VR; some failures come from the limitations of technology, but many come from a lack of understanding perception, interaction, design principles, and real users. This book discusses these issues by emphasizing the human element of VR. The fact is, if we do not get the human element correct, then no amount of technology will make VR anything more than an interesting tool confined to research laboratories. Even when VR principles are fully understood, the first implementation is rarely novel and almost never ideal due to the complex nature of VR and the countless possibilities that can be created. The VR principles discussed in this book will enable readers to intelligently experiment with the rules and iteratively design towards innovative experiences.},
	publisher = {Association for Computing Machinery and Morgan \& Claypool},
	author = {Jerald, Jason},
	month = sep,
	year = {2015},
}


@book{dorner_virtual_2019,
	address = {Berlin, Heidelberg},
	title = {Virtual und {Augmented} {Reality} ({VR}/{AR}): {Grundlagen} und {Methoden} der {Virtuellen} und {Augmentierten} {Realität}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-662-58860-4 978-3-662-58861-1},
	shorttitle = {Virtual und {Augmented} {Reality} ({VR}/{AR})},
	url = {http://link.springer.com/10.1007/978-3-662-58861-1},
	language = {de},
	urldate = {2024-08-01},
	publisher = {Springer},
	editor = {Dörner, Ralf and Broll, Wolfgang and Grimm, Paul and Jung, Bernhard},
	year = {2019},
	doi = {10.1007/978-3-662-58861-1},
	keywords = {Augmented Reality, Augmentierte Realität, Buch Virtuelle und Augmentierte Realität, Computer Graphics, Computer Vision, Computergrafik, Grundlagen, Human Computer Interaction, Interactive Systems, Selbststudium, Studierende, Technologie, Virtual Reality, Virtual und Augmented Reality (VR / AR), Virtuelle Realität, VR},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\QSHRCENN\\Dörner et al. - 2019 - Virtual und Augmented Reality (VRAR) Grundlagen .pdf:application/pdf},
}

@article{steriadis_designing_2003,
	title = {Designing human-computer interfaces for quadriplegic people},
	volume = {10},
	issn = {1073-0516},
	url = {https://doi.org/10.1145/772047.772049},
	doi = {10.1145/772047.772049},
	abstract = {The need for participation in an emerging Information Society has led to several research efforts for designing accessibility solutions for disabled people. In this paper we present a method for developing Human-Computer Interfaces (HCIs) for quadriplegic people in modern programming environments. The presented method accommodates the design of scanning interfaces with modern programming tools, leading to flexible interfaces with improved appearance and it is based on the use of specially designed software objects called "wifsids" (Widgets For Single-switch Input Devices). The wifsid structure is demonstrated and 4 types of wifsids are analyzed. Developed software applications are to be operated by single-switch activations that are captured through the wifsids, with the employment of several modes of the scanning technique. We also demonstrate the "Autonomia" software application, that has been developed according to the specific methodology. The basic snapshots of this application are analyzed, in order to demonstrate how the wifsids cooperate with the scanning process in a user-friendly environment that enables a quadriplegic person to access an ordinary computer system.},
	number = {2},
	urldate = {2024-07-24},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	author = {Steriadis, Constantine E. and Constantinou, Philip},
	month = jun,
	year = {2003},
	pages = {87--118},
}


@misc{apple_einfuhrung_2024,
	title = {Einführung in die {Funktionen} der {Bedienungshilfen} auf dem {iPhone}},
	url = {https://support.apple.com/de-de/guide/iphone/iph3e2e4367/ios},
	abstract = {Das iPhone umfasst Funktionen zur Bedienungshilfe, von denen du in den Bereichen Sehen, Interaktion, Hören und hinsichtlich anderer kognitiver Anforderungen profitieren kannst.},
	language = {de},
	urldate = {2024-06-28},
	journal = {Apple Support},
	author = {{Apple}},
	year = {2024},
	file = {Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\SRWRHF83\\ios.html:text/html},
}

@misc{noauthor_tips_nodate,
	title = {Tips for using {Switch} {Access} - {Android} {Accessibility} {Help}},
	url = {https://support.google.com/accessibility/android/answer/6395627?hl=en&ref_topic=6151780&sjid=16986929134949672065-EU},
	urldate = {2024-08-05},
    author = {Google},
    year = {2024},
	file = {Tips for using Switch Access - Android Accessibility Help:C\:\\Users\\Finja\\Zotero\\storage\\UY7RXL49\\6395627.html:text/html},
}

@article{wohlgenannt_virtual_2020,
	title = {Virtual {Reality}},
	volume = {62},
	issn = {1867-0202},
	url = {https://doi.org/10.1007/s12599-020-00658-9},
	doi = {10.1007/s12599-020-00658-9},
	language = {en},
	number = {5},
	urldate = {2024-09-11},
	journal = {Business \& Information Systems Engineering},
	author = {Wohlgenannt, Isabell and Simons, Alexander and Stieglitz, Stefan},
	month = oct,
	year = {2020},
	keywords = {Augmented reality, Extended reality, Immersive systems, Mixed reality, Virtual reality},
	pages = {455--461},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\RAXKZ2AU\\Wohlgenannt et al. - 2020 - Virtual Reality.pdf:application/pdf},
}

@article{walsh_virtual_2002,
	title = {Virtual {Reality}: {A} {Technology} in {Need} of {IS} {Research}},
	volume = {8},
	issn = {15293181},
	shorttitle = {Virtual {Reality}},
	url = {https://aisel.aisnet.org/cais/vol8/iss1/20},
	doi = {10.17705/1CAIS.00820},
	abstract = {Although virtual reality (VR) technology has been available since the 1970’s, it is becoming increasingly sophisticated and cost effective. Architecture, education, medicine, electronic commerce, collaboration, and data visualization are some of areas where VR is beginning to be applied. Much of the reported research on VR is technological rather than social, leaving only a limited understanding of its behavioral and organizational impacts and its potential for novel situations. Immersion, interactivity, and presence are intriguing concepts that emerged as important to VR research, but are yet ill-defined. In this paper we argue that the information system research community offers a unique and valuable perspective on VR research, and that this capability represents a logical extension of the work in several IS research domains. Multimethodological approaches using both positivist and emergent perspectives are needed. A research framework that can be used to begin this work is described.},
	language = {en},
	urldate = {2024-09-11},
	journal = {Communications of the Association for Information Systems},
	author = {Walsh, Kenneth R. and Pawlowski, Suzanne D.},
	year = {2002},
	file = {Walsh und Pawlowski - 2002 - Virtual Reality A Technology in Need of IS Resear.pdf:C\:\\Users\\Finja\\Zotero\\storage\\R8PB6T9Y\\Walsh und Pawlowski - 2002 - Virtual Reality A Technology in Need of IS Resear.pdf:application/pdf},
}

@article{witmer_measuring_1998,
	title = {Measuring {Presence} in {Virtual} {Environments}: {A} {Presence} {Questionnaire}},
	volume = {7},
	shorttitle = {Measuring {Presence} in {Virtual} {Environments}},
	url = {https://doi.org/10.1162/105474698565686},
	doi = {10.1162/105474698565686},
	abstract = {The effectiveness of virtual environments (VEs) has often been linked to the sense of presence reported by users of those VEs. (Presence is defined as the subjective experience of being in one place or environment, even when one is physically situated in another.) We believe that presence is a normal awareness phenomenon that requires directed attention and is based in the interaction between sensory stimulation, environmental factors that encourage involvement and enable immersion, and internal tendencies to become involved. Factors believed to underlie presence were described in the premier issue of Presence: Teleoperators and Virtual Environments. We used these factors and others as the basis for a presence questionnaire (PQ) to measure presence in VEs. In addition we developed an immersive tendencies questionnaire (ITQ) to measure differences in the tendencies of individuals to experience presence. These questionnaires are being used to evaluate relationships among reported presence and other research variables. Combined results from four experiments lead to the following conclusions: the PQ and ITQ are internally consistent measures with high reliability;there is a weak but consistent positive relation between presence and task performance in VEs;individual tendencies as measured by the ITQ predict presence as measured by the PQ; andindividuals who report more simulator sickness symptoms in VE report less presence than those who report fewer symptoms.},
	number = {3},
	urldate = {2023-06-25},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {Witmer, Bob G. and Singer, Michael J.},
	month = jun,
	year = {1998},
	pages = {225--240},
	file = {Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\M5TQTGJ5\\Measuring-Presence-in-Virtual-Environments-A.html:text/html},
}

@article{sanchez-vives_presence_2005,
	title = {From presence to consciousness through virtual reality},
	volume = {6},
	copyright = {2005 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn1651},
	doi = {10.1038/nrn1651},
	abstract = {Immersive virtual environments can break the deep, everyday connection between where our senses tell us we are and where we are actually located and whom we are with. The concept of 'presence' refers to the phenomenon of behaving and feeling as if we are in the virtual world created by computer displays. In this article, we argue that presence is worthy of study by neuroscientists, and that it might aid the study of perception and consciousness.},
	language = {en},
	number = {4},
	urldate = {2024-09-11},
	journal = {Nature Reviews Neuroscience},
	author = {Sanchez-Vives, Maria V. and Slater, Mel},
	month = apr,
	year = {2005},
	note = {Publisher: Nature Publishing Group},
	keywords = {Animal Genetics and Genomics, Behavioral Sciences, Biological Techniques, Biomedicine, general, Neurobiology, Neurosciences},
	pages = {332--339},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\D6UVXX7V\\Sanchez-Vives und Slater - 2005 - From presence to consciousness through virtual rea.pdf:application/pdf},
}

@inproceedings{milgram_augmented_1995,
	address = {Boston, MA},
	title = {Augmented reality: a class of displays on the reality-virtuality continuum},
	shorttitle = {Augmented reality},
	url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=981543},
	doi = {10.1117/12.197321},
	abstract = {In this paper we discuss Augmented Reality (AR) displays in a general sense, within the context of a Reality-Virtuality (RV) continuum, encompassing a large class of "Mixed Reality" (MR) displays, which also includes Augmented Virtuality (AV). MR displays are defined by means of seven examples of existing display concepts in which real objects and virtual objects are juxtaposed. Essential factors which distinguish different Mixed Reality display systems from each other are presented, first by means of a table in which the nature of the underlying scene, how it is viewed, and the observer's reference to it are compared, and then by means of a three dimensional taxonomic framework, comprising: Extent of World Knowledge (EWK), Reproduction Fidelity (RF) and Extent of Presence Metaphor (EPM). A principal objective of the taxonomy is to clarify terminology issues and to provide a framework for classifying research across different disciplines.},
	language = {en},
	urldate = {2023-06-25},
	author = {Milgram, Paul and Takemura, Haruo and Utsumi, Akira and Kishino, Fumio},
	editor = {Das, Hari},
	month = dec,
	year = {1995},
	pages = {282--292},
	file = {Milgram et al. - 1995 - Augmented reality a class of displays on the real.pdf:C\:\\Users\\Finja\\Zotero\\storage\\7PFGARJ8\\Milgram et al. - 1995 - Augmented reality a class of displays on the real.pdf:application/pdf},
}

@article{somrak_estimating_2019,
	title = {Estimating {VR} {Sickness} and user experience using different {HMD} technologies: {An} evaluation study},
	volume = {94},
	issn = {0167739X},
	shorttitle = {Estimating {VR} {Sickness} and user experience using different {HMD} technologies},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X18325044},
	doi = {10.1016/j.future.2018.11.041},
	abstract = {This paper presents results of a user study of the effects of virtual reality technology on VR Sickness and User Experience. In our study the participants watched two different panoramic (360) videos, one with relaxing content (beach clip) and second one with action content (roller coaster video clip). Videos were watched on four different head mounted displays (HMDs) and on the 2D television as a reference display. To assess VR Sickness discomfort levels, we have used the Simulator Sickness Questionnaire (SSQ), and for user experience the User Experience Questionnaire (UEQ) was used. For quick assessments of VR Sickness discomfort levels, we have also used Subjective Units of Distress Scale (SUDS). We have found a strong correlation between SUDS and total SSQ score and between total SSQ score and SSQ-D score. Shown negative correlation between VR Sickness discomfort levels (assessed by SSQ and UEQ Questionnaire), and user experience (assessed by UEQ Questionnaire), indicates that presence of VR Sickness symptoms affects the user experience.},
	language = {en},
	urldate = {2023-04-14},
	journal = {Future Generation Computer Systems},
	author = {Somrak, Andrej and Humar, Iztok and Hossain, M. Shamim and Alhamid, Mohammed F. and Hossain, M. Anwar and Guna, Jože},
	month = may,
	year = {2019},
	pages = {302--316},
	file = {Somrak et al. - 2019 - Estimating VR Sickness and user experience using d.pdf:C\:\\Users\\Finja\\Zotero\\storage\\LCN6DJA3\\Somrak et al. - 2019 - Estimating VR Sickness and user experience using d.pdf:application/pdf},
}

@article{saredakis_factors_2020,
	title = {Factors {Associated} {With} {Virtual} {Reality} {Sickness} in {Head}-{Mounted} {Displays}: {A} {Systematic} {Review} and {Meta}-{Analysis}},
	volume = {14},
	issn = {1662-5161},
	shorttitle = {Factors {Associated} {With} {Virtual} {Reality} {Sickness} in {Head}-{Mounted} {Displays}},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2020.00096/full},
	doi = {10.3389/fnhum.2020.00096},
	abstract = {{\textless}p{\textgreater}The use of head-mounted displays (HMD) for virtual reality (VR) application-based purposes including therapy, rehabilitation, and training is increasing. Despite advancements in VR technologies, many users still experience sickness symptoms. VR sickness may be influenced by technological differences within HMDs such as resolution and refresh rate, however, VR content also plays a significant role. The primary objective of this systematic review and meta-analysis was to examine the literature on HMDs that report Simulator Sickness Questionnaire (SSQ) scores to determine the impact of content. User factors associated with VR sickness were also examined. A systematic search was conducted according to PRISMA guidelines. Fifty-five articles met inclusion criteria, representing 3,016 participants (mean age range 19.5–80; 41\% female). Findings show gaming content recorded the highest total SSQ mean 34.26 (95\%CI 29.57–38.95). VR sickness profiles were also influenced by visual stimulation, locomotion and exposure times. Older samples (mean age ≥35 years) scored significantly lower total SSQ means than younger samples, however, these findings are based on a small evidence base as a limited number of studies included older users. No sex differences were found. Across all types of content, the pooled total SSQ mean was relatively high 28.00 (95\%CI 24.66–31.35) compared with recommended SSQ cut-off scores. These findings are of relevance for informing future research and the application of VR in different contexts.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-07-25},
	journal = {Frontiers in Human Neuroscience},
	author = {Saredakis, Dimitrios and Szpak, Ancret and Birckhead, Brandon and Keage, Hannah A. D. and Rizzo, Albert and Loetscher, Tobias},
	month = mar,
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Cybersickness, Teils gelesen, virtual reality, virtual environment, head-mounted display, Simulator sickness},
	file = {Volltext:C\:\\Users\\Finja\\Zotero\\storage\\5682VW33\\Saredakis et al. - 2020 - Factors Associated With Virtual Reality Sickness i.pdf:application/pdf},
}

@article{kennedy_simulator_1993,
	title = {Simulator {Sickness} {Questionnaire}: {An} {Enhanced} {Method} for {Quantifying} {Simulator} {Sickness}},
	volume = {3},
	issn = {1050-8414},
	shorttitle = {Simulator {Sickness} {Questionnaire}},
	url = {https://doi.org/10.1207/s15327108ijap0303_3},
	doi = {10.1207/s15327108ijap0303_3},
	abstract = {Simulator sickness (SS) in high-fidelity visual simulators is a byproduct of modem simulation technology. Although it involves symptoms similar to those of motion-induced sickness (MS), SS tends to be less severe, to be of lower incidence, and to originate from elements of visual display and visuo-vestibular interaction atypical of conditions that induce MS. Most studies of SS to date index severity with some variant of the Pensacola Motion Sickness Questionnaire (MSQ). The MSQ has several deficiencies as an instrument for measuring SS. Some symptoms included in the scoring of MS are irrelevant for SS, and several are misleading. Also, the configural approach of the MSQ is not readily adaptable to computer administration and scoring. This article describes the development of a Simulator Sickness Questiomaire (SSQ), derived from the MSQ using a series of factor analyses, and illustrates its use in monitoring simulator performance with data from a computerized SSQ survey of 3,691 simulator hops. The database used for development included more than 1,100 MSQs, representing data from 10 Navy simulators. The SSQ provides straightforward computer or manual scoring, increased power to identify "problem" simulators, and improved diagnostic capability.},
	number = {3},
	urldate = {2023-06-25},
	journal = {The International Journal of Aviation Psychology},
	author = {Kennedy, Robert S. and Lane, Norman E. and Berbaum, Kevin S. and Lilienthal, Michael G.},
	month = jul,
	year = {1993},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1207/s15327108ijap0303\_3},
	pages = {203--220},
}

@article{kennedy_research_2010,
	series = {Special {Section} - {The} {First} {International} {Symposium} on {Visually} {Induced} {Motion} {Sickness}, {Fatigue}, and {Photosensitive} {Epileptic} {Seizures} ({VIMS2007})},
	title = {Research in visually induced motion sickness},
	volume = {41},
	issn = {0003-6870},
	url = {https://www.sciencedirect.com/science/article/pii/S0003687009001574},
	doi = {10.1016/j.apergo.2009.11.006},
	abstract = {While humans have experienced motion sickness symptoms in response to inertial motion from early history through the present day, motion sickness symptoms also occur from exposure to some types of visual displays. Even in the absence of physical motion, symptoms may result from visually perceived motion, which are often classified as effects of visually induced motion sickness (VIMS). This paper provides a brief discussion of general motion sickness and then reviews findings from three lines of recent VIMS investigations that we have conducted.},
	number = {4},
	urldate = {2024-09-23},
	journal = {Applied Ergonomics},
	author = {Kennedy, Robert S. and Drexler, Julie and Kennedy, Robert C.},
	month = jul,
	year = {2010},
	keywords = {Cybersickness, Motion sickness, Simulator sickness, Sopite Syndrome, Visual displays, Visually induced motion sickness},
	pages = {494--503},
	file = {ScienceDirect Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\BX9ZDGJ2\\S0003687009001574.html:text/html},
}

@article{oman_motion_1990,
	title = {Motion sickness: a synthesis and evaluation of the sensory conflict theory},
	volume = {68},
	issn = {0008-4212},
	shorttitle = {Motion sickness},
	url = {https://cdnsciencepub.com/doi/abs/10.1139/y90-044},
	doi = {10.1139/y90-044},
	abstract = {"Motion sickness" is the general term describing a group of common nausea syndromes originally attributed to motion-induced cerebral ischemia, stimulation of abdominal organ afferents, or overstimulation of the vestibular organs of the inner ear. Seasickness, car sickness, and airsickness are commonly experienced examples. However, the identification of other variants such as spectacle sickness and flight simulator sickness in which the physical motion of the head and body is normal or even absent has led to a succession of "sensory conflict" theories that offer a more comprehensive etiologic perspective. Implicit in the conflict theory is the hypothesis that neural and (or) humoral signals originate in regions of the brain subserving spatial orientation, and that these signals somehow traverse to other centers mediating sickness symptoms. Unfortunately, our present understanding of the neurophysiological basis of motion sickness is incomplete. No sensory conflict neuron or process has yet been physiologically identified. This paper reviews the types of stimuli that cause sickness and synthesizes a mathematical statement of the sensory conflict hypothesis based on observer theory from control engineering. A revised mathematical model is presented that describes the dynamic coupling between the putative conflict signals and nausea magnitude estimates. Based on the model, what properties would a conflict neuron be expected to have?Key words: motion sickness, nausea, vestibular, vision, mathematical models.},
	number = {2},
	urldate = {2024-09-23},
	journal = {Canadian Journal of Physiology and Pharmacology},
	author = {Oman, Charles M.},
	month = feb,
	year = {1990},
	note = {Publisher: NRC Research Press},
	pages = {294--303},
}

@article{laviola_discussion_2000,
	title = {A discussion of cybersickness in virtual environments},
	volume = {32},
	issn = {0736-6906},
	url = {https://dl.acm.org/doi/10.1145/333329.333344},
	doi = {10.1145/333329.333344},
	abstract = {An important and troublesome problem with current virtual environment (VE) technology is the tendency for some users to exhibit symptoms that parallel symptoms of classical motion sickness both during and after the VE experience. This type of sickness, cybersickness, is distinct from motion sickness in that the user is often stationary but has a compelling sense of self motion through moving visual imagery. Unfortunately, there are many factors that can cause cybersickness and there is no foolproof method for eliminating the problem. In this paper, I discuss a number of the primary factors that contribute to the cause of cybersickness, describe three conflicting cybersickness theories that have been postulated, and discuss some possible methods for reducing cybersickness in VEs.},
	language = {en},
	number = {1},
	urldate = {2024-09-24},
	journal = {ACM SIGCHI Bulletin},
	author = {LaViola, Joseph J.},
	month = jan,
	year = {2000},
	pages = {47--56},
	file = {LaViola - 2000 - A discussion of cybersickness in virtual environme.pdf:C\:\\Users\\Finja\\Zotero\\storage\\WBM8MVZP\\LaViola - 2000 - A discussion of cybersickness in virtual environme.pdf:application/pdf},
}

@article{riccio_ecological_1991,
	title = {An ecological {Theory} of {Motion} {Sickness} and {Postural} {Instability}},
	volume = {3},
	issn = {1040-7413},
	url = {https://doi.org/10.1207/s15326969eco0303_2},
	doi = {10.1207/s15326969eco0303_2},
	abstract = {In this article we present a new theory of motion sickness. In the sensory conflict theory, changes in stimulation of perceptual systems are believed to be responsible for motion sickness. We discuss the fact that these changes in stimulation are not independent of the animal-environment interaction, but are determined by corresponding changes in the constraints operating on the control of action. Thus, provocative situations may be characterized by novel demands on the control of action as well as by novel patterns of stimulation. Our hypothesis is that animals become sick in situations in which they do not possess (or have not yet learned) strategies that are effective for the maintenance of postural stability. We identify a broad range of situations over which the occurrence of motion sickness is related to factors that should influence postural stability. This allows us to establish a logical link between motion sickness and postural stability. Our analysis implies that an understanding of stability should be an important part of the agenda in research on perception and action in general. We suggest that postural instability could be related to the concept of dynamical disease which has been developed in the literature on nonlinear physiological control systems. We conclude with suggestions for research based on the new approach.},
	number = {3},
	urldate = {2024-09-24},
	journal = {Ecological Psychology},
	author = {Riccio, Gary E. and Stoffregen, Thomas A.},
	month = sep,
	year = {1991},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1207/s15326969eco0303\_2},
	pages = {195--240},
}


@inproceedings{gerling_critical_2021,
	address = {New York, NY, USA},
	series = {{CHI} '21},
	title = {A {Critical} {Examination} of {Virtual} {Reality} {Technology} in the {Context} of the {Minority} {Body}},
	isbn = {978-1-4503-8096-6},
	url = {https://doi.org/10.1145/3411764.3445196},
	doi = {10.1145/3411764.3445196},
	abstract = {Virtual Reality (VR) holds the promise of immersing people in virtual worlds. However, initial work on the relationship between VR and disability suggests that VR is a body-centric technology that poses barriers for disabled users. We supplement this work with a theoretical analysis of immersive VR through the lens of Surrogate Body theory, a concept from media theory for the structured examination of interactive media in use. Leveraging Critical Disability Studies, particularly the theory of the Minority Body, we explore the assumptions about bodies inherent in VR, and we reflect on implications of these assumptions when disabled people engage with the technology. Our findings show that VR is an inherently ableist technology that assumes a ‘corporeal standard’ (i.e., an ‘ideal’, non-disabled human body), and fails to adequately accommodate disabled people. We conclude with implications for HCI research on VR, and discuss design approaches that foster inclusive technology development.},
	urldate = {2024-03-28},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Gerling, Kathrin and Spiel, Katta},
	month = may,
	year = {2021},
	keywords = {Virtual reality, Disability Studies, Surrogate Body Theory, Körper/Motorisch, Hilfreich},
	pages = {1--14},
}

@inproceedings{mott_i_2020,
	address = {Virtual Event Greece},
	title = {“{I} just went into it assuming that {I} wouldn't be able to have the full experience”: {Understanding} the {Accessibility} of {Virtual} {Reality} for {People} with {Limited} {Mobility}},
	isbn = {978-1-4503-7103-2},
	shorttitle = {“{I} just went into it assuming that {I} wouldn't be able to have the full experience”},
	url = {https://dl.acm.org/doi/10.1145/3373625.3416998},
	doi = {10.1145/3373625.3416998},
	abstract = {Virtual reality (VR) has the potential to transform many aspects of our daily lives, including work, entertainment, communication, and education. However, there has been little research into understanding the usability of VR for people with mobility limitations. In this paper, we present the results of an exploration to understand the accessibility of VR for people with limited mobility. We conducted semi-structured interviews with 16 people with limited mobility about their thoughts on, and experiences with, VR systems. We identifed 7 barriers related to the physical accessibility of VR devices that people with limited mobility might encounter, ranging from the initial setup of a VR system to keeping VR controllers in view of cameras embedded in VR headsets. We also elicited potential improvements to VR systems that would address some accessibility concerns. Based on our fndings, we discuss the importance of considering the abilities of people with limited mobility when designing VR systems, as the abilities of many participants did not match the assumptions embedded in the design of current VR systems.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {Proceedings of the 22nd {International} {ACM} {SIGACCESS} {Conference} on {Computers} and {Accessibility}},
	publisher = {ACM},
	author = {Mott, Martez and Tang, John and Kane, Shaun and Cutrell, Edward and Ringel Morris, Meredith},
	month = oct,
	year = {2020},
	keywords = {Virtual reality, Körper/Motorisch},
	pages = {1--13},
	file = {Mott et al. - 2020 - “I just went into it assuming that I wouldn't be a.pdf:C\:\\Users\\Finja\\Zotero\\storage\\VPJBWZC2\\Mott et al. - 2020 - “I just went into it assuming that I wouldn't be a.pdf:application/pdf},
}

@techreport{wong_survey_2017,
	title = {Survey for {People} with {Disabilities}},
	language = {en},
	author = {Wong, Alice and Gillis, Hannah and Peck, Ben},
	year = {2017},
	file = {Wong et al. - Survey for People with Disabilities.pdf:C\:\\Users\\Finja\\Zotero\\storage\\DUE4539Y\\Wong et al. - Survey for People with Disabilities.pdf:application/pdf},
}

@inproceedings{dombrowski_designing_2019,
	address = {Cham},
	title = {Designing {Inclusive} {Virtual} {Reality} {Experiences}},
	isbn = {978-3-030-21607-8},
	doi = {10.1007/978-3-030-21607-8_3},
	abstract = {Virtual Reality experiences are often touted as having the most immersive user experience possible outside of live action, yet Virtual Reality (VR) technology is often not designed to be inclusive to all users. From mild sim sickness effects to a lack of accessible controls, many users are being left out from the modern virtual reality experience. There are however tools available to the developer that will make inclusive VR design a possibility.},
	language = {en},
	booktitle = {Virtual, {Augmented} and {Mixed} {Reality}. {Multimodal} {Interaction}},
	publisher = {Springer International Publishing},
	author = {Dombrowski, Matt and Smith, Peter A. and Manero, Albert and Sparkman, John},
	editor = {Chen, Jessie Y.C. and Fragomeni, Gino},
	year = {2019},
	keywords = {Virtual reality},
	pages = {33--43},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\CKHGUV3L\\Dombrowski et al. - 2019 - Designing Inclusive Virtual Reality Experiences.pdf:application/pdf},
}

@inproceedings{mott_accessible_2019,
	title = {Accessible by {Design}: {An} {Opportunity} for {Virtual} {Reality}},
	shorttitle = {Accessible by {Design}},
	url = {https://ieeexplore.ieee.org/document/8951960},
	doi = {10.1109/ISMAR-Adjunct.2019.00122},
	abstract = {Too often, the accessibility of technology to people with disabilities is an afterthought (if it is considered at all); post-hoc or third-party patches to accessibility, while better than no solution, are less optimal than interface designs that consider ability-based concerns from the start [31]. Virtual Reality (VR) technologies are at a crucial point of near-maturity, with emerging, but not yet widespread, commercialization; as such, VR technologies have an opportunity to integrate accessibility as a fundamental, developing cross-industry standards and guidelines to ensure high-quality, inclusive experiences that could revolutionize the power and reach of this medium. In this position paper, we discuss the needs, opportunities, and challenges of creating accessible VR.},
	urldate = {2024-04-02},
	booktitle = {2019 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} {Adjunct} ({ISMAR}-{Adjunct})},
	author = {Mott, Martez and Cutrell, Edward and Gonzalez Franco, Mar and Holz, Christian and Ofek, Eyal and Stoakley, Richard and Ringel Morris, Meredith},
	month = oct,
	year = {2019},
	keywords = {Virtual reality, Headphones, Three-dimensional displays, Standards, Sinnvoll nach Abstract, Hardware, Metadata, Rendering (computer graphics), Virtual-Reality,-Mixed-Reality,-accessibility, Hilfreich},
	pages = {451--454},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Finja\\Zotero\\storage\\8S72EEIM\\8951960.html:text/html;Mott et al. - 2019 - Accessible by Design An Opportunity for Virtual R.pdf:C\:\\Users\\Finja\\Zotero\\storage\\34QJL22C\\Mott et al. - 2019 - Accessible by Design An Opportunity for Virtual R.pdf:application/pdf},
}

@inproceedings{minakata_pointing_2019,
	address = {New York, NY, USA},
	series = {{ETRA} '19},
	title = {Pointing by gaze, head, and foot in a head-mounted display},
	isbn = {978-1-4503-6709-7},
	url = {https://doi.org/10.1145/3317956.3318150},
	doi = {10.1145/3317956.3318150},
	abstract = {This paper presents a Fitts' law experiment and a clinical case study performed with a head-mounted display (HMD). The experiment compared gaze, foot, and head pointing. With the equipment setup we used, gaze was slower than the other pointing methods, especially in the lower visual field. Throughputs for gaze and foot pointing were lower than mouse and head pointing and their effective target widths were also higher. A follow-up case study included seven participants with movement disorders. Only two of the participants were able to calibrate for gaze tracking but all seven could use head pointing, although with throughput less than one-third of the non-clinical participants.},
	urldate = {2024-05-13},
	booktitle = {Proceedings of the 11th {ACM} {Symposium} on {Eye} {Tracking} {Research} \& {Applications}},
	publisher = {Association for Computing Machinery},
	author = {Minakata, Katsumi and Hansen, John Paulin and MacKenzie, I. Scott and Bækgaard, Per and Rajanna, Vijay},
	month = jun,
	year = {2019},
	keywords = {virtual reality, disability, accessibility, head-mounted displays, Körper/Motorisch, dwell activation, Fitts' law, foot interaction, gaze interaction, hand controller, head interaction, ISO 9241-9},
	pages = {1--9},
	file = {Volltext:C\:\\Users\\Finja\\Zotero\\storage\\JWTMGK7F\\Minakata et al. - 2019 - Pointing by gaze, head, and foot in a head-mounted.pdf:application/pdf},
}

@inproceedings{wang_intelligent_2018,
	address = {Taichung, Taiwan},
	title = {Intelligent {Wearable} {Virtual} {Reality} ({VR}) {Gaming} {Controller} for {People} with {Motor} {Disabilities}},
	isbn = {978-1-5386-9269-1},
	url = {https://ieeexplore.ieee.org/document/8613653/},
	doi = {10.1109/AIVR.2018.00034},
	abstract = {Current interaction methods for VR/AR headsets need hand-held devices/joysticks, head-mounted cameras or external sensors to detect hand and body movements in order to generate actionable commands. It is not feasible for the people with motor or movement disabilities. In the meanwhile, the current assistive technologies need complicated, cumbersome, and expensive equipment, which are not user-friendly, not portable, and still require certain motor control capabilities from the user. Our approach aims to bridge this gap by developing a compact, non-obtrusive and ergonomic wearable device, to measure physiological signals associated with human eye/facial gestures, and therefore generate simple and intuitive commands to interact with the VR/AR headset. Our innovation uses machine learning and non-invasive biosensors attached on top of the ears (temple positions) to identify eye movements and facial expressions. Through proper design of Human-Machine interactions, we have come up with several engaging humangame multimodal interaction methods by simply moving user’s eyes and doing facial expressions. It allows people with motor impairment have a fun time to play VR games with other people, totally “hand-free”.},
	language = {en},
	urldate = {2024-05-14},
	booktitle = {2018 {IEEE} {International} {Conference} on {Artificial} {Intelligence} and {Virtual} {Reality} ({AIVR})},
	publisher = {IEEE},
	author = {Wang, Ker-Jiun and Liu, Quanbo and Zhao, Yifan and Zheng, Caroline Yan and Vhasure, Soumya and Liu, Quanfeng and Thakur, Prakash and Sun, Mingui and Mao, Zhi-Hong},
	month = dec,
	year = {2018},
	keywords = {Virtual reality, Körper/Motorisch},
	pages = {161--164},
	file = {Wang et al. - 2018 - Intelligent Wearable Virtual Reality (VR) Gaming C.pdf:C\:\\Users\\Finja\\Zotero\\storage\\AU2FMCZS\\Wang et al. - 2018 - Intelligent Wearable Virtual Reality (VR) Gaming C.pdf:application/pdf},
}

@misc{noauthor_paneovr_nodate,
	title = {{PaneoVR} – {Immersive} {Video} {Training} {Toolkit}},
	url = {https://paneovr.net/},
	language = {de},
	urldate = {2025-01-03},
	file = {Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\GUTHAUGF\\paneovr.net.html:text/html},
}

@misc{noauthor_virdipa-_nodate,
	title = {{ViRDiPA}- Über das {Projekt} {\textbar} {Hochschule} {Bielefeld} ({HSBI})},
	url = {https://www.hsbi.de/inbvg/projekte/bildungsforschung/virtual-reality-basierte-digital-reusable-learning-objects-in-der-pflegeausbildung/ueber-dasprojekt},
	urldate = {2025-01-03},
	file = {ViRDiPA- Über das Projekt | Hochschule Bielefeld (HSBI):C\:\\Users\\Finja\\Zotero\\storage\\AFZD5V2R\\ueber-dasprojekt.html:text/html},
}

@article{stanney_cybersickness_1997,
	title = {Cybersickness is {Not} {Simulator} {Sickness}},
	volume = {41},
	issn = {1071-1813},
	url = {https://doi.org/10.1177/107118139704100292},
	doi = {10.1177/107118139704100292},
	abstract = {Factor analysis of a large number of motion sickness self-reports from exposure to military flight simulators revealed three separate clusters of symptoms. Based on this analysis a symptom profile emerged for simulators where Oculomotor symptoms predominated, followed by Nausea and least by Disorientation-like symptoms. Current users of virtual environment (VE) systems have also begun to report varying degrees of what they are calling cybersickness, which initially appeared to be similar to simulator sickness. We have found, after examination of eight experiments using different VE systems, that the profile of cybersickness is sufficiently different from simulator sickness — with Disorientation being the predominant symptom and Oculomotor the least. The total severity of cybersickness was also found to be approximately three times greater than that of simulator sickness. Perhaps these different strains of motion sickness may provide insight into the different causes of the two maladies.},
	language = {en},
	number = {2},
	urldate = {2025-01-04},
	journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
	author = {Stanney, Kay M. and Kennedy, Robert S. and Drexler, Julie M.},
	month = oct,
	year = {1997},
	note = {Publisher: SAGE Publications Inc},
	pages = {1138--1142},
}

@inproceedings{bimberg_usage_2020,
	address = {Atlanta, GA, USA},
	title = {On the {Usage} of the {Simulator} {Sickness} {Questionnaire} for {Virtual} {Reality} {Research}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72816-532-5},
	url = {https://ieeexplore.ieee.org/document/9090573/},
	doi = {10.1109/VRW50115.2020.00098},
	abstract = {Originally developed for military ﬂight simulators in the 1990s, the Simulator Sickness Questionnaire (SSQ) has been widely adopted to quantify sickness elicited by modern virtual reality systems. We illustrate the inherent challenges of applying the SSQ in virtual reality research and highlight large differences that can be found in related work. Based on our observations, we conclude by providing suggestions on how to improve the expressiveness of SSQ results in future studies and encourage researchers to consider simpler measurement methods if their research questions allow.},
	language = {en},
	urldate = {2024-12-07},
	booktitle = {2020 {IEEE} {Conference} on {Virtual} {Reality} and {3D} {User} {Interfaces} {Abstracts} and {Workshops} ({VRW})},
	publisher = {IEEE},
	author = {Bimberg, Pauline and Weissker, Tim and Kulik, Alexander},
	month = mar,
	year = {2020},
	keywords = {Hilfreich},
	pages = {464--467},
	file = {Bimberg et al. - 2020 - On the Usage of the Simulator Sickness Questionnai.pdf:C\:\\Users\\Finja\\Zotero\\storage\\7V4MTGI4\\Bimberg et al. - 2020 - On the Usage of the Simulator Sickness Questionnai.pdf:application/pdf},
}

@incollection{brooke_sus_1996,
	title = {{SUS} - {A} quick and dirty usability scale},
	abstract = {Usability does not exist in any absolute sense; it can only be defined with reference to particular contexts. This, in turn, means that there are no absolute measures of usability, since, if the usability of an artefact is defined by the context in which that artefact is used, measures of usability must of necessity be defined by that context too. Despite this, there is a need for broad general measures which can be used to compare usability across a range of contexts. In addition, there is a need for “quick and dirty” methods to allow low cost assessments of usability in industrial systems evaluation. This chapter describes the System Usability Scale (SUS) a reliable, low-cost usability scale that can be used for global assessments of systems usability.},
	language = {en},
	booktitle = {P. {W}. {Jordan}, {B}. {Thomas}, {B}. {A}. {Weermeester} und {McClelland} ({Hg}.): {Usability} {Evaluation} in {Industry}},
	publisher = {London: Taylor \& Francis},
	author = {Brooke, John},
	year = {1996},
	pages = {189--194},
	file = {Brooke - SUS - A quick and dirty usability scale.pdf:C\:\\Users\\Finja\\Zotero\\storage\\BLL6E24E\\Brooke - SUS - A quick and dirty usability scale.pdf:application/pdf},
}

@article{bangor_empirical_2008,
	title = {An {Empirical} {Evaluation} of the {System} {Usability} {Scale}},
	volume = {24},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447310802205776},
	doi = {10.1080/10447310802205776},
	abstract = {This article presents nearly 10 year's worth of System Usability Scale (SUS) data collected on numerous products in all phases of the development lifecycle. The SUS, developed by Brooke (1996), reflected a strong need in the usability community for a tool that could quickly and easily collect a user's subjective rating of a product's usability. The data in this study indicate that the SUS fulfills that need. Results from the analysis of this large number of SUS scores show that the SUS is a highly robust and versatile tool for usability professionals. The article presents these results and discusses their implications, describes nontraditional uses of the SUS, explains a proposed modification to the SUS to provide an adjective rating that correlates with a given score, and provides details of what constitutes an acceptable SUS score.},
	number = {6},
	urldate = {2025-01-04},
	journal = {International Journal of Human–Computer Interaction},
	author = {Bangor, Aaron and Kortum, Philip T. and Miller, James T.},
	month = jul,
	year = {2008},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447310802205776},
	pages = {574--594},
}

@book{laugwitz_construction_2008,
	title = {Construction and {Evaluation} of a {User} {Experience} {Questionnaire}},
	volume = {5298},
	isbn = {978-3-540-89349-3},
	abstract = {An end-user questionnaire to measure user experience quickly in a simple and immediate way while covering a preferably comprehensive impression of the product user experience was the goal of the reported construction process. An empirical approach for the item selection was used to ensure practical relevance of items. Usability experts collected terms and statements on user experience and usability, including ‘hard’ as well as ‘soft’ aspects. These statements were consolidated and transformed into a first questionnaire version containing 80 bipolar items. It was used to measure the user experience of software products in several empirical studies. Data were subjected to a factor analysis which resulted in the construction of a 26 item questionnaire including the six factors Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and Novelty. Studies conducted for the original German questionnaire and an English version indicate a satisfactory level of reliability and construct validity.},
	author = {Laugwitz, Bettina and Held, Theo and Schrepp, Martin},
	month = nov,
	year = {2008},
	doi = {10.1007/978-3-540-89350-9_6},
	note = {Journal Abbreviation: USAB 2008
Pages: 76
Publication Title: USAB 2008},
}

@article{noauthor_din_2020,
	title = {{DIN} {EN} {ISO} 9241-210},
	journal = {Ergonomie der Mensch-System-Interaktion - Teil 210: Menschzentrierte Gestaltung interaktiver Systeme},
	year = {2020},
	file = {DIN_EN_ISO_9241-210.pdf:C\:\\Users\\Finja\\Zotero\\storage\\H2S6Z8JA\\DIN_EN_ISO_9241-210.pdf:application/pdf},
}

@misc{noauthor_din_nodate-1,
	title = {{DIN} {EN} {ISO} 9241-11:2018-11, {Ergonomie} der {Mensch}-{System}-{Interaktion}\_- {Teil}\_11: {Gebrauchstauglichkeit}: {Begriffe} und {Konzepte} ({ISO}\_9241-11:2018); {Deutsche} {Fassung} {EN}\_ISO\_9241-11:2018},
	shorttitle = {{DIN} {EN} {ISO} 9241-11},
	url = {https://www.dinmedia.de/de/-/-/279590417},
	doi = {10.31030/2757945},
	urldate = {2024-08-16},
	publisher = {DIN Media GmbH},
	file = {DIN_EN_ISO_9241-11_(2018).pdf:C\:\\Users\\Finja\\Zotero\\storage\\66QZGYAG\\DIN_EN_ISO_9241-11_(2018).pdf:application/pdf},
}

@misc{noauthor_din_nodate,
	title = {{DIN} {EN} {ISO} 9241-110:2020-10, {Ergonomie} der {Mensch}-{System}-{Interaktion}\_- {Teil}\_110: {Interaktionsprinzipien} ({ISO}\_9241-110:2020); {Deutsche} {Fassung} {EN}\_ISO\_9241-110:2020},
	shorttitle = {{DIN} {EN} {ISO} 9241-110},
	url = {https://www.dinmedia.de/de/-/-/320862700},
	doi = {10.31030/3147467},
	urldate = {2024-08-16},
	publisher = {DIN Media GmbH},
}

@techreport{DINISO9241-110,
  type       = {DIN},
  key        = {DIN EN ISO 9241-110:2020-10},
  title      = {{DIN EN ISO 9241-110:2020-10, Ergonomie der
                Mensch-System-Interaktion - Teil 110:Interaktionsprinzipien}},
  shorttitle = {{DIN EN ISO 9241-110:2020-10}},
  volume     = 2020,
  month      = 3,
  publisher  = {Beuth Verlag, Berlin}
}

@techreport{DINISO9241,
  type       = {DIN},
  key        = {DIN EN ISO 9421-210:2020-03},
  title      = {{DIN EN ISO 9421-210:2020-03, Ergonomie der
                Mensch-System-Interaktion - Teil 210: Menschzentrierte Gestaltung interaktiver
                Systeme}},
  shorttitle = {{DIN EN ISO 9421-210:2020-03}},
  volume     = 2020,
  month      = 3,
  publisher  = {Beuth Verlag, Berlin}
}

@techreport{DINISO9241-11,
  type       = {DIN},
  key        = {DIN EN ISO 9241-11:2018-11},
  title      = {{DIN EN ISO 9241-11:2018-11, Ergonomie der Mensch-System-Interaktion - Teil 11: Gebrauchstauglichkeit: Begriffe und Konzepte}},
  shorttitle = {{DIN EN ISO 9241-11:2018-11}},
  volume     = 2018,
  month      = 5,
  publisher  = {Beuth Verlag, Berlin}
}


@inproceedings{hassenzahl_user_2008,
	address = {Metz France},
	title = {User experience ({UX}): towards an experiential perspective on product quality},
	isbn = {978-1-60558-285-6},
	shorttitle = {User experience ({UX})},
	url = {https://dl.acm.org/doi/10.1145/1512714.1512717},
	doi = {10.1145/1512714.1512717},
	abstract = {User Experience (UX) is not just "old wine in new bottles". It is a truly extended and distinct perspective on the quality of interactive technology: away from products and problems to humans and the drivers of positive experience. This paper will present my particular perspective on UX and will discuss its implications for the field of Human-Computer Interaction.},
	language = {en},
	urldate = {2023-06-22},
	booktitle = {Proceedings of the 20th {Conference} on l'{Interaction} {Homme}-{Machine}},
	publisher = {ACM},
	author = {Hassenzahl, Marc},
	month = sep,
	year = {2008},
	pages = {11--15},
	file = {Hassenzahl - 2008 - User experience (UX) towards an experiential pers.pdf:C\:\\Users\\Finja\\Zotero\\storage\\2Y5XMMFJ\\Hassenzahl - 2008 - User experience (UX) towards an experiential pers.pdf:application/pdf},
}

@inproceedings{alexandrovsky_evaluating_2021,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '21},
	title = {Evaluating {User} {Experiences} in {Mixed} {Reality}},
	isbn = {978-1-4503-8095-9},
	url = {https://doi.org/10.1145/3411763.3441337},
	doi = {10.1145/3411763.3441337},
	urldate = {2023-06-16},
	booktitle = {Extended {Abstracts} of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Alexandrovsky, Dmitry and Putze, Susanne and Schwind, Valentin and Mekler, Elisa D. and Smeddinck, Jan David and Kahl, Denise and Krüger, Antonio and Malaka, Rainer},
	month = may,
	year = {2021},
	keywords = {Sinnvoll nach Abstract},
	pages = {1--5},
	file = {Eingereichte Version:C\:\\Users\\Finja\\Zotero\\storage\\2MBW8IT6\\Alexandrovsky et al. - 2021 - Evaluating User Experiences in Mixed Reality.pdf:application/pdf},
}

@article{kim_systematic_2020,
	title = {A {Systematic} {Review} of a {Virtual} {Reality} {System} from the {Perspective} of {User} {Experience}},
	volume = {36},
	issn = {1044-7318},
	url = {https://doi.org/10.1080/10447318.2019.1699746},
	doi = {10.1080/10447318.2019.1699746},
	abstract = {Virtual reality (VR) is receiving attention enough to be considered as its revival age in both industrial and academic field. Since VR systems have various types of interaction with users and new types of interaction are constantly being developed, various studies investigating user experience (UX) of VR systems are continuously needed. However, there is still a lack of research on the taxonomy that can recognize the main characteristics of VR system at a glance by reflecting the influencing factors of UX. Therefore, we collected and reviewed the research related to the UX evaluation of the VR system in order to identify the current research status and to suggest future research direction. To achieve this, a systematic review was conducted on UX studies for VR, and taxonomies of VR system including influencing factors of UX were proposed. A total of 393 unique articles were collected, and 65 articles were selected to be reviewed via Systematic Reviews and Meta-Analyses methodology. The selected articles were analyzed according to predefined taxonomies. As a result, current status of research can be identified base on the proposed taxonomies. Besides, issues related to VR devices and technology, and research method for future research directions can be suggested.},
	number = {10},
	urldate = {2023-06-16},
	journal = {International Journal of Human–Computer Interaction},
	author = {Kim, Yong Min and Rhiu, Ilsun and Yun, Myung Hwan},
	month = jun,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10447318.2019.1699746},
	keywords = {Qualität bestanden, Sinnvoll nach Abstract},
	pages = {893--910},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\TZ2DS7F5\\Kim et al. - 2020 - A Systematic Review of a Virtual Reality System fr.pdf:application/pdf},
}

@article{heilemann_accessibility_2021,
	title = {Accessibility {Guidelines} for {VR} {Games} - {A} {Comparison} and {Synthesis} of a {Comprehensive} {Set}},
	volume = {2},
	issn = {2673-4192},
	url = {https://www.frontiersin.org/articles/10.3389/frvir.2021.697504},
	abstract = {Increasing numbers of gamers worldwide have led to more attention being paid to accessibility in games. Virtual Reality offers new possibilities to enable people to play games but also comes with new challenges for accessibility. Guidelines provide help for developers to avoid barriers and include persons with disabilities in their games. As of today, there are only a few extensive collections of accessibility rules on video games. Especially new technologies like virtual reality are sparsely represented in current guidelines. In this work, we provide an overview of existing guidelines for games and VR applications. We examine the most relevant resources, and form a union set. From this, we derive a comprehensive set of guidelines. This set summarizes the rules that are relevant for accessible VR games. We discuss the state of guidelines and their implication on the development of educational games, and provide suggestions on how to improve the situation.},
	urldate = {2024-01-11},
	journal = {Frontiers in Virtual Reality},
	author = {Heilemann, Fiona and Zimmermann, Gottfried and Münster, Patrick},
	year = {2021},
	keywords = {Virtual reality, Gelesen und super},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\4DREW5SR\\Heilemann et al. - 2021 - Accessibility Guidelines for VR Games - A Comparis.pdf:application/pdf},
}

@misc{meta_meta_2024,
	title = {Meta {Quest} {Virtual} {Reality} {Check} ({VRC}) {Guidelines} {\textbar} {Oculus} {Developers}},
	url = {https://developer.oculus.com/resources/publish-quest-req/},
	urldate = {2024-04-17},
	author = {{Meta}},
	month = oct,
	year = {2024},
	file = {Meta Quest Virtual Reality Check (VRC) Guidelines | Oculus Developers:C\:\\Users\\Finja\\Zotero\\storage\\C8EMIYGK\\publish-quest-req.html:text/html},
}

@misc{xr_access_-_virtual_augmented__mixed_reality_for_people_with_disabilities_xr_2024,
	title = {{XR} {Access}},
	url = {https://xraccess.org/about/},
	abstract = {About XR Access
XR Access is a research consortium based at Cornell Tech, the New York City campus of Cornell University. It was founded in 2019 by Prof. Shiri Azenkot of Cornell Tech and Larry Goldberg, the former head of accessibility at Verizon Media.
XR Access fosters and leads a community that engages, connects, and influences},
	language = {en-US},
	urldate = {2024-04-17},
	journal = {XR Access},
	author = {{XR Access - Virtual, Augmented \& Mixed Reality for People with Disabilities}},
	year = {2024},
	file = {Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\FFS82WMW\\about.html:text/html},
}

@article{berg_industry_2017,
	title = {Industry use of virtual reality in product design and manufacturing: a survey},
	volume = {21},
	issn = {1434-9957},
	shorttitle = {Industry use of virtual reality in product design and manufacturing},
	url = {https://doi.org/10.1007/s10055-016-0293-9},
	doi = {10.1007/s10055-016-0293-9},
	abstract = {In 1999, Fred Brooks, virtual reality pioneer and Professor of Computer Science at the University of North Carolina at Chapel Hill, published a seminal paper describing the current state of virtual reality (VR) technologies and applications (Brooks in IEEE Comput Graph Appl 19(6):16, 1999). Through his extensive survey of industry, Brooks concluded that virtual reality had finally arrived and “barely works”. His report included a variety of industries which leveraged these technologies to support industry-level innovation. Virtual reality was being employed to empower decision making in design, evaluation, and training processes across multiple disciplines. Over the past two decades, both industrial and academic communities have contributed to a large knowledge base on numerous virtual reality topics. Technical advances have enabled designers and engineers to explore and interact with data in increasingly natural ways. Sixteen years have passed since Brooks original survey. Where are we now? The research presented here seeks to describe the current state of the art of virtual reality as it is used as a decision-making tool in product design, particularly in engineering-focused businesses. To this end, a survey of industry was conducted over several months spanning fall 2014 and spring 2015. Data on virtual reality applications across a variety of industries was gathered through a series of on-site visits. In total, on-site visits with 18 companies using virtual reality were conducted as well as remote conference calls with two others. The authors interviewed 62 people across numerous companies from varying disciplines and perspectives. Success stories and existing challenges were highlighted. While virtual reality hardware has made considerable strides, unique attention was given to applications and the associated decisions that they support. Results suggest that virtual reality has arrived: it works! It is mature, stable, and, most importantly, usable. VR is actively being used in a number of industries to support decision making and enable innovation. Insights from this survey can be leveraged to help guide future research directions in virtual reality technology and applications.},
	language = {en},
	number = {1},
	urldate = {2025-01-05},
	journal = {Virtual Reality},
	author = {Berg, Leif P. and Vance, Judy M.},
	month = mar,
	year = {2017},
	keywords = {Artificial Intelligence, Virtual Environment, Virtual Reality, Virtual Reality Application, Virtual Reality System, Virtual Reality Technology},
	pages = {1--17},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\DHHR4ZHY\\Berg und Vance - 2017 - Industry use of virtual reality in product design .pdf:application/pdf},
}

@article{bowman_virtual_2007,
	title = {Virtual {Reality}: {How} {Much} {Immersion} {Is} {Enough}?},
	volume = {40},
	issn = {1558-0814},
	shorttitle = {Virtual {Reality}},
	url = {https://ieeexplore.ieee.org/abstract/document/4287241},
	doi = {10.1109/MC.2007.257},
	abstract = {Solid evidence of virtual reality's benefits has graduated from impressive visual demonstrations to producing results in practical applications. Further, a realistic experience is no longer immersion's sole asset. Empirical studies show that various components of immersion provide other benefits - full immersion is not always necessary. The goal of immersive virtual environments (VEs) was to let the user experience a computer-generated world as if it were real - producing a sense of presence, or "being there," in the user's mind.},
	number = {7},
	urldate = {2025-01-05},
	journal = {Computer},
	author = {Bowman, Doug A. and McMahan, Ryan P.},
	month = jul,
	year = {2007},
	note = {Conference Name: Computer},
	keywords = {3D visualization, Application software, Cities and towns, Clinical trials, Conference management, Costs, immersion, Management training, Medical treatment, Military computing, Public speaking, virtual reality, Virtual reality},
	pages = {36--43},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Finja\\Zotero\\storage\\YMMMM5H8\\4287241.html:text/html},
}

@book{jerald_vr_2016,
	title = {The {VR} {Book}: {Human}-{Centered} {Design} for {Virtual} {Reality}},
	isbn = {978-1-970001-13-6},
	shorttitle = {The {VR} {Book}},
	abstract = {This is a strong foundation of human-centric virtual reality design for anyone and everyone involved in creating VR experiences. Without a clear understanding of the human side of virtual reality (VR), the experience will always fail.  The VR Book bridges this gap by focusing on human-centered design. Creating compelling VR applications is an incredibly complex challenge. When done well, these experiences can be brilliant and pleasurable, but when done badly, they can result in frustration and sickness. Whereas limitations of technology can cause bad VR execution, problems are oftentimes caused by a lack of understanding human perception, interaction, design principles, and real users. This book focuses on the human elements of VR, such as how users perceive and intuitively interact with various forms of reality, causes of VR sickness, creating useful and pleasing content, and how to design and iterate upon effective VR applications.  This book is not just for VR designers, it is for managers, programmers, artists, psychologists, engineers, students, educators, and user experience professionals. It is for the entire VR team, as everyone contributing should understand at least the basics of the many aspects of VR design. The industry is rapidly evolving, and The VR Book stresses the importance of building prototypes, gathering feedback, and using adjustable processes to efficiently iterate towards success. It contains extensive details on the most important aspects of VR, more than 600 applicable guidelines, and over 300 additional references.},
	language = {en},
	publisher = {Morgan \& Claypool},
	author = {Jerald, Jason},
	month = sep,
	year = {2016},
	note = {Google-Books-ID: ZEBiDwAAQBAJ},
	keywords = {Computers / Design, Graphics \& Media / Video \& Animation, Computers / Human-Computer Interaction (HCI), Computers / Virtual \& Augmented Reality},
}

@incollection{barfield_presence_1995,
	address = {USA},
	title = {Presence and performance within virtual environments},
	isbn = {978-0-19-507555-7},
	urldate = {2025-01-05},
	booktitle = {Virtual environments and advanced interface design},
	publisher = {Oxford University Press, Inc.},
	author = {Barfield, Woodrow and Zeltzer, David and Sheridan, Thomas and Slater, Mel},
	month = jun,
	year = {1995},
	pages = {473--513},
}

@article{girvan_what_2018,
	title = {What is a virtual world? {Definition} and classification},
	volume = {66},
	issn = {1556-6501},
	shorttitle = {What is a virtual world?},
	url = {https://doi.org/10.1007/s11423-018-9577-y},
	doi = {10.1007/s11423-018-9577-y},
	abstract = {In 2008, articles by Bell and Schroeder provided an initial platform from which to develop a coherent definition of the term ‘virtual worlds’. Yet over the past ten years, there has been little development of the term. Instead there is confusion in the literature, with the introduction of new terms which are at times used to classify the type of virtual world and at others are used synonymously with the term. At the same time there has been a resurgence of interest in the potential of virtual reality which further muddies the conceptual waters. While the lack of a clear and common understanding of a term is not uncommon, there are implications for researchers and practitioners. To address these issues, this paper presents a new framework for the definition of virtual worlds, arguing what it is for a world to be virtual, the user experience that is a necessary part of this and the technical features which afford this. For the first time the relationships between commonly confused terms and technologies are identified to provide a much needed conceptual clarity for researchers and educators.},
	language = {en},
	number = {5},
	urldate = {2025-01-05},
	journal = {Educational Technology Research and Development},
	author = {Girvan, Carina},
	month = oct,
	year = {2018},
	keywords = {3D learning environment, Definition, Digital Education and Educational Technology, MUVE, Virtual world},
	pages = {1087--1100},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\3T62JJAV\\Girvan - 2018 - What is a virtual world Definition and classifica.pdf:application/pdf},
}

@article{fast-berglund_testing_2018,
	series = {Proceedings of the 8th {Swedish} {Production} {Symposium} ({SPS} 2018)},
	title = {Testing and validating {Extended} {Reality} ({xR}) technologies in manufacturing},
	volume = {25},
	issn = {2351-9789},
	url = {https://www.sciencedirect.com/science/article/pii/S2351978918305730},
	doi = {10.1016/j.promfg.2018.06.054},
	abstract = {xR technologies such as Augmented Reality (AR) and Virtual Reality (VR) are increasing at shop-floors but are still in need for validation in order to make good strategic decisions regarding implementation. The areas where most industries apply AR is within remote guidance and complex tasks such as maintenance. VR is mostly used for layout planning and more and more for virtual training. This paper aims to present results from six case studies with over two hundred responders from academy and industry about usage and strategies on where and when to implement what xR technology..},
	urldate = {2025-01-05},
	journal = {Procedia Manufacturing},
	author = {Fast-Berglund, Åsa and Gong, Liang and Li, Dan},
	month = jan,
	year = {2018},
	keywords = {AR, CPPS-testbed, cyber-physical, information levels, learning factories, utilization, VR},
	pages = {31--38},
	file = {ScienceDirect Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\3WUZM57V\\S2351978918305730.html:text/html},
}

@article{kim_virtual_2018,
	title = {Virtual reality sickness questionnaire ({VRSQ}): {Motion} sickness measurement index in a virtual reality environment},
	volume = {69},
	issn = {0003-6870},
	shorttitle = {Virtual reality sickness questionnaire ({VRSQ})},
	url = {https://www.sciencedirect.com/science/article/pii/S000368701730282X},
	doi = {10.1016/j.apergo.2017.12.016},
	abstract = {This study aims to develop a motion sickness measurement index in a virtual reality (VR) environment. The VR market is in an early stage of market formation and technological development, and thus, research on the side effects of VR devices such as simulator motion sickness is lacking. In this study, we used the simulator sickness questionnaire (SSQ), which has been traditionally used for simulator motion sickness measurement. To measure the motion sickness in a VR environment, 24 users performed target selection tasks using a VR device. The SSQ was administered immediately after each task, and the order of work was determined using the Latin square design. The existing SSQ was revised to develop a VR sickness questionnaire, which is used as the measurement index in a VR environment. In addition, the target selection method and button size were found to be significant factors that affect motion sickness in a VR environment. The results of this study are expected to be used for measuring and designing simulator sickness using VR devices in future studies.},
	urldate = {2024-07-25},
	journal = {Applied Ergonomics},
	author = {Kim, Hyun K. and Park, Jaehyun and Choi, Yeongcheol and Choe, Mungyeong},
	month = may,
	year = {2018},
	keywords = {Virtual reality, Motion sickness, Simulator sickness questionnaire},
	pages = {66--73},
	file = {ScienceDirect Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\CKCLKRZ2\\S000368701730282X.html:text/html},
}

@article{mccauley_cybersickness_1992,
	title = {Cybersickness: {Perception} of {Self}-{Motion} in {Virtual} {Environments}},
	volume = {1},
	shorttitle = {Cybersickness},
	url = {https://doi.org/10.1162/pres.1992.1.3.311},
	doi = {10.1162/pres.1992.1.3.311},
	abstract = {Human perceptual systems have evolved to provide accurate information about orientation and movement through the environment. However, these systems have been challenged in the past century by modern transportation devices and will be further challenged by virtual environments (VEs) and teleoperator systems. Illusory self-motion within a VE (“cyberspace”) will be entertaining and instructive, but for many users it will result in motion sickness (“cybersickness”). Sensory conflict theory and the poison hypothesis provide an unproven theoretical foundation for understanding the phenomenon. Although no single engineering solution is likely, the problem can be contained by a combination of engineering design, equipment calibration, and exposure management.},
	number = {3},
	urldate = {2025-01-05},
	journal = {Presence: Teleoperators and Virtual Environments},
	author = {McCauley, Michael E. and Sharkey, Thomas J.},
	month = aug,
	year = {1992},
	pages = {311--318},
	file = {Snapshot:C\:\\Users\\Finja\\Zotero\\storage\\CPP9TMIE\\Cybersickness-Perception-of-Self-Motion-in-Virtual.html:text/html},
}


@article{creed_inclusive_2023,
	title = {Inclusive {AR}/{VR}: accessibility barriers for immersive technologies},
	volume = {23},
	issn = {1615-5297},
	shorttitle = {Inclusive {AR}/{VR}},
	url = {https://doi.org/10.1007/s10209-023-00969-0},
	doi = {10.1007/s10209-023-00969-0},
	abstract = {Augmented and virtual reality (AR/VR) hold significant potential to transform how we communicate, collaborate, and interact with others. However, there has been a lack of work to date investigating accessibility barriers in relation to immersive technologies for people with disabilities. To address current gaps in knowledge, we led two multidisciplinary sandpits with key stakeholders (including academic researchers, AR/VR industry specialists, people with lived experience of disability, assistive technologists, and representatives from national charities and special needs colleges) to collaboratively explore and identify existing challenges with AR and VR experiences. We present key themes that emerged from sandpit activities and map out the interaction barriers identified across a spectrum of impairments (including physical, cognitive, visual, and auditory disabilities). We conclude with recommendations for future work addressing the challenges highlighted to support the development of more inclusive AR and VR experiences.},
	language = {en},
	urldate = {2024-10-07},
	journal = {Universal Access in the Information Society},
	author = {Creed, Chris and Al-Kalbani, Maadh and Theil, Arthur and Sarcar, Sayan and Williams, Ian},
	year = {2023},
	keywords = {Augmented reality, Virtual reality, Artificial Intelligence, Inclusive design, Accessibility, Hilfreich},
	pages = {59-73},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\VJ985Y5H\\Creed et al. - 2024 - Inclusive ARVR accessibility barriers for immers.pdf:application/pdf},
}

@inproceedings{valakou_framework_2024,
	address = {Cham},
	title = {A {Framework} for {Accessibility} in {XR} {Environments}},
	isbn = {978-3-031-49215-0},
	doi = {10.1007/978-3-031-49215-0_31},
	abstract = {Digital accessibility is vital for ensuring equal access and usability for individuals with disabilities. However, addressing the unique challenges faced by individuals with disabilities in XR environments remains a complex task. This paper presents an ongoing accessibility framework designed to empower developers in creating inclusive XR applications. The framework aims to provide a comprehensive solution, addressing the needs of individuals with disabilities, by incorporating various accessibility features, based on XR accessibility guidelines, best practices, and state of the art approaches. The current version of the framework has focused on the accessibility of XR environments for blind or partially sighted users, enhancing their interaction with text, images, videos, and 3D artefacts. The proposed work lays the foundation for Extended Reality (XR) developers to easily encompass accessible assets. In this respect, it offers customizable text settings, alternative visual content text, and multiple user interaction control mechanisms. Furthermore, it includes features such as edge enhancement, interactive element descriptions with dynamic widgets, scanning for navigation, and foreground positioning of active objects. The framework also supports scene adaptations upon user demand to cater to specific visual needs.},
	language = {en},
	booktitle = {{HCI} {International} 2023 – {Late} {Breaking} {Posters}},
	publisher = {Springer Nature Switzerland},
	author = {Valakou, Aikaterini and Margetis, George and Ntoa, Stavroula and Stephanidis, Constantine},
	editor = {Stephanidis, Constantine and Antona, Margherita and Ntoa, Stavroula and Salvendy, Gavriel},
	year = {2024},
	keywords = {Augmented Reality, Virtual Reality, Inclusion, Accessibility, Hilfreich, Extended Reality, Framework},
	pages = {252--263},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\FAUIKNFA\\Valakou et al. - 2024 - A Framework for Accessibility in XR Environments.pdf:application/pdf},
}

@article{creed_inclusive_2024_2,
	title = {Inclusive {Augmented} and {Virtual} {Reality}: {A} {Research} {Agenda}},
	volume = {40},
  number = {20},
	issn = {1044-7318},
	shorttitle = {Inclusive {Augmented} and {Virtual} {Reality}},
	url = {https://doi.org/10.1080/10447318.2023.2247614},
	doi = {10.1080/10447318.2023.2247614},
	abstract = {Augmented and virtual reality experiences present significant barriers for disabled people, making it challenging to fully engage with immersive platforms. Whilst researchers have started to explore potential solutions addressing these accessibility issues, we currently lack a comprehensive understanding of research areas requiring further investigation to support the development of inclusive AR/VR systems. To address current gaps in knowledge, we led a series of multidisciplinary sandpits with relevant stakeholders (i.e., academic researchers, industry specialists, people with lived experience of disability, assistive technologists, and representatives from disability organisations, charities, and special needs educational institutions) to collaboratively explore research challenges, opportunities, and solutions. Based on insights shared by participants, we present a research agenda identifying key areas where further work is required in relation to specific forms of disability (i.e., across the spectrum of physical, visual, cognitive, and hearing impairments), including wider considerations associated with the development of more accessible immersive platforms.},
	urldate = {2024-10-07},
	journal = {International Journal of Human–Computer Interaction},
	author = {Creed, Chris and Al-Kalbani, Maadh and Theil, Arthur and Sarcar, Sayan and Williams, Ian},
	year = {2024},
	keywords = {virtual reality, augmented reality, Accessibility, Hilfreich, inclusive design},
	pages = {6200-6219},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\T9Y9S4IZ\\Creed et al. - Inclusive Augmented and Virtual Reality A Researc.pdf:application/pdf},
}

@article{yuan_game_2011,
	title = {Game accessibility: a survey},
	volume = {10},
	issn = {1615-5297},
	url = {https://doi.org/10.1007/s10209-010-0189-5},
	doi = {10.1007/s10209-010-0189-5},
	abstract = {Over the last three decades, video games have evolved from a pastime into a force of change that is transforming the way people perceive, learn about, and interact with the world around them. In addition to entertainment, games are increasingly used for other purposes such as education or health. Despite this increased interest, a significant number of people encounter barriers when playing games due to a disability. Accessibility problems may include the following: (1) not being able to receive feedback; (2) not being able to determine in-game responses; (3) not being able to provide input using conventional input devices. This paper surveys the current state-of-the-art in research and practice in the accessibility of video games and points out relevant areas for future research. A generalized game interaction model shows how a disability affects ones ability to play games. Estimates are provided on the total number of people in the United States whose ability to play games is affected by a disability. A large number of accessible games are surveyed for different types of impairments, across several game genres, from which a number of high- and low-level accessibility strategies are distilled for game developers to inform their design.},
	number = {1},
	journal = {Universal Access in the Information Society},
	author = {Yuan, Bei and Folmer, Eelke and Harris, Frederick C.},
	month = mar,
	year = {2011},
	pages = {81-100},
}

@article{Weber22111996,
author = {Debbie Weber and Maryann Demchak},
title = {Using Assistive Technology with Individuals with Severe Disabilities},
journal = {Computers in the Schools},
volume = {12},
number = {3},
pages = {43-56},
year = {1996},
publisher = {Routledge},
doi = {10.1300/J025v12n03\_06},
URL = {https://doi.org/10.1300/J025v12n03_06},
eprint = {https://doi.org/10.1300/J025v12n03_06},
}

@article{10.1145/772047.772049,
author = {Steriadis, Constantine E. and Constantinou, Philip},
title = {Designing human-computer interfaces for quadriplegic people},
year = {2003},
issue_date = {June 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/772047.772049},
doi = {10.1145/772047.772049},
abstract = {The need for participation in an emerging Information Society has led to several research efforts for designing accessibility solutions for disabled people. In this paper we present a method for developing Human-Computer Interfaces (HCIs) for quadriplegic people in modern programming environments. The presented method accommodates the design of scanning interfaces with modern programming tools, leading to flexible interfaces with improved appearance and it is based on the use of specially designed software objects called "wifsids" (Widgets For Single-switch Input Devices). The wifsid structure is demonstrated and 4 types of wifsids are analyzed. Developed software applications are to be operated by single-switch activations that are captured through the wifsids, with the employment of several modes of the scanning technique. We also demonstrate the "Autonomia" software application, that has been developed according to the specific methodology. The basic snapshots of this application are analyzed, in order to demonstrate how the wifsids cooperate with the scanning process in a user-friendly environment that enables a quadriplegic person to access an ordinary computer system.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = jun,
pages = {87–118},
numpages = {32},
keywords = {word-prediction, wifsid, single-switch input, scanning selection, quadriplegic people, mouse simulation, motor-impaired users, graphical keyboard, disability, augmentative communications, assistive technology, Accessibility}
}

@article{Blackstien-Adler30062004,
author = {Susie Blackstien-Adler and Fraser Shein and Janet Quintal and Shae Birch and Patrice L. Tamar Weiss},
title = {Mouse Manipulation Through Single-Switch Scanning},
journal = {Assistive Technology},
volume = {16},
number = {1},
pages = {28-42},
year = {2004},
publisher = {Taylor \& Francis},
doi = {10.1080/10400435.2004.10132072},

    note ={PMID: 15359467},
URL = { 
    
        https://doi.org/10.1080/10400435.2004.10132072
},
eprint = { 
    
        https://doi.org/10.1080/10400435.2004.10132072
},
}

@incollection{COOK2015117,
title = {Chapter 6 - Making the Connection: User Inputs for Assistive Technologies},
editor = {Albert M. Cook and Janice M. Polgar},
booktitle = {Assistive Technologies (Fourth Edition)},
publisher = {Mosby},
edition = {Fourth Edition},
address = {St. Louis (MO)},
pages = {117-138},
year = {2015},
isbn = {978-0-323-09631-7},
doi = {https://doi.org/10.1016/B978-0-323-09631-7.00006-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323096317000065},
author = {Albert M. Cook and Janice M. Polgar}
}

@article{Simpson30062007,
author = {Richard Simpson, Heidi Koester and Ed LoPresti},
title = {Selecting an Appropriate Scan Rate: The “.65 Rule”},
journal = {Assistive Technology},
volume = {19},
number = {2},
pages = {51-60},
year = {2007},
publisher = {Taylor \& Francis},
doi = {10.1080/10400435.2007.10131865},

    note ={PMID: 17727073},
URL = { 
    
        https://doi.org/10.1080/10400435.2007.10131865 
},
eprint = { 
    
        https://doi.org/10.1080/10400435.2007.10131865
},
}

@article{ciccone_next_2023,
	title = {The {Next} {Generation} of {Virtual} {Reality}: {Recommendations} for {Accessible} and {Ergonomic} {Design}},
	volume = {31},
	issn = {1064-8046},
	shorttitle = {The {Next} {Generation} of {Virtual} {Reality}},
	url = {https://doi.org/10.1177/10648046211002578},
	doi = {10.1177/10648046211002578},
	abstract = {Virtual reality (VR) device prices drastically declined in the past decade, leading to a shift in the VR market. As VR enters the mainstream, the design of both VR hardware and software must evolve to meet the needs of diverse users. Current systems appeal to niche consumers who are willing and able to use VR despite bulky hardware and accessibility barriers in software. The current review outlines the limitations of current VR systems and proposes ways in which designs can be iterated to produce more ergonomic and accessible systems for a broader range of users.},
	language = {en},
	number = {2},
	urldate = {2024-04-17},
	journal = {Ergonomics in Design},
	author = {Ciccone, Brendan A. and Bailey, Shannon K. T. and Lewis, Joanna E.},
	month = apr,
	year = {2023},
	note = {Publisher: SAGE Publications Inc},
	keywords = {Virtual reality},
	pages = {24--27},
	file = {SAGE PDF Full Text:C\:\\Users\\Finja\\Zotero\\storage\\2Z5BR9HK\\Ciccone et al. - 2023 - The Next Generation of Virtual Reality Recommenda.pdf:application/pdf},
}

@article{dudley_inclusive_2023,
	title = {Inclusive {Immersion}: a review of efforts to improve accessibility in virtual reality, augmented reality and the metaverse},
	volume = {27},
	issn = {1434-9957},
	shorttitle = {Inclusive {Immersion}},
	url = {https://doi.org/10.1007/s10055-023-00850-8},
	doi = {10.1007/s10055-023-00850-8},
	abstract = {Virtual Reality (VR) and Augmented Reality (AR) afford new forms of work and leisure. While affordable and effective VR and AR headsets are now available, neither technology has achieved widespread user adoption. However, we predict continual technological advances and cost reductions are likely to lead to wider diffusion in society. Bridging the chasm from the early adopters to the early majority will require careful consideration of the needs of a more casual and diverse user population. In particular, it is desirable to minimise the exclusion of potential users based on their unique needs and maximise the inclusion of users in these novel immersive experiences. Ensuring equitable access to the emerging metaverse further reinforces the need to consider the diverse needs of users. We refer to this objective of maximising the accessibility and enjoyment potential of users of VR, AR and the metaverse as Inclusive Immersion. This paper reviews the research and commercial landscape seeking to address the accessibility needs of users in VR and AR. The survey provides the basis for a synthesis of the emerging strategies for maximising the inclusiveness of VR and AR applications. Finally, we identify several unaddressed accessibility challenges requiring further research attention. Our paper consolidates disparate efforts related to promoting accessible VR and AR and delivers directions for advancing research in this area.},
	language = {en},
	number = {4},
	urldate = {2024-04-10},
	journal = {Virtual Reality},
	author = {Dudley, John and Yin, Lulu and Garaj, Vanja and Kristensson, Per Ola},
	month = dec,
	year = {2023},
	keywords = {Augmented Reality, Mixed Reality, Virtual reality, Virtual Reality, augmented reality, Accessibility, Disability, The metaverse, Hilfreich},
	pages = {2989--3020},
	file = {Full Text PDF:C\:\\Users\\Finja\\Zotero\\storage\\H9DH3M9Z\\Dudley et al. - 2023 - Inclusive Immersion a review of efforts to improv.pdf:application/pdf},
}